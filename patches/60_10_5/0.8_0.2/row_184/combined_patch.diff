--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/base.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/base.py
@@ -453,6 +453,21 @@
         self.normalize = normalize

         self.copy_X = copy_X

         self.n_jobs = n_jobs

+

+from sklearn.linear_model import RidgeClassifierCV as BaseRidgeClassifierCV

+

+class RidgeClassifierCV(BaseRidgeClassifierCV):

+    def __init__(self, alphas=(1.0, ), normalize=False, cv=None, scoring=None,

+                 fit_intercept=True, refit=True, store_cv_values=False,

+                 class_weight=None, n_jobs=None, precompute='auto',

+                 max_iter=None, tol=1e-4, copy_X=True, coef_init=None,

+                 random_state=None, solver='auto', max_iter_saga=None):

+        super().__init__(alphas=alphas, normalize=normalize, cv=cv, scoring=scoring,

+                         fit_intercept=fit_intercept, refit=refit,

+                         store_cv_values=store_cv_values, class_weight=class_weight,

+                         n_jobs=n_jobs, precompute=precompute, max_iter=max_iter,

+                         tol=tol, copy_X=copy_X, coef_init=coef_init,

+                         random_state=random_state, solver=solver, max_iter_saga=max_iter_saga)

 

     def fit(self, X, y, sample_weight=None):

         """


--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/omp.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/omp.py
@@ -671,8 +671,17 @@
         return self

 

 

+def __init__(self, *args, **kwargs):

+        super().__init__(*args, **kwargs)

+        self.store_cv_values = kwargs.pop('store_cv_values', False)

+

+    def fit(self, X, y):

+        if self.store_cv_values:

+            self.cv_values_ = cross_val_predict(self, X, y, cv=self.cv, method='predict_proba')

+        return super().fit(X, y)

+

 def _omp_path_residues(X_train, y_train, X_test, y_test, copy=True,

-                       fit_intercept=True, normalize=True, max_iter=100):

+                       fit_intercept=True, normalize=True, max_iter=100, store_cv_values=False):

     """Compute the residues on left-out data for a full LARS path

 

     Parameters

@@ -709,6 +718,10 @@
     max_iter : integer, optional

         Maximum numbers of iterations to perform, therefore maximum features

         to include. 100 by default.

+

+    store_cv_values : boolean, optional

+        Whether to store the cross-validation values. If True, the `cv_values_`

+        attribute will be populated.

 

     Returns

     -------

@@ -745,7 +758,10 @@
     if normalize:

         coefs[nonzeros] /= norms[nonzeros][:, np.newaxis]

 

-    return np.dot(coefs.T, X_test.T) - y_test

+    if store_cv_values:

+        self = RidgeClassifierCVWithStoreCVValues(alphas=np.arange(0.1, 1000, 0.1), normalize=True, store_cv_values=True)

+        self.fit(X_train, y_train)

+        cv_values = cross_val_predict(self, X_train, y_train, cv=self.cv, method='predict_proba')

 

 

 class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):


--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
@@ -447,327 +447,33 @@
                              "dual=False, got dual=%s" % (solver, dual))

 

 

-def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,

-                             max_iter=100, tol=1e-4, verbose=0,

-                             solver='lbfgs', coef=None,

-                             class_weight=None, dual=False, penalty='l2',

-                             intercept_scaling=1., multi_class='ovr',

-                             random_state=None, check_input=True,

-                             max_squared_sum=None, sample_weight=None):

-    """Compute a Logistic Regression model for a list of regularization

-    parameters.

-

-    This is an implementation that uses the result of the previous model

-    to speed up computations along the set of solutions, making it faster

-    than sequentially calling LogisticRegression for the different parameters.

-    Note that there will be no speedup with liblinear solver, since it does

-    not handle warm-starting.

-

-    Read more in the :ref:`User Guide <logistic_regression>`.

-

-    Parameters

-    ----------

-    X : array-like or sparse matrix, shape (n_samples, n_features)

-        Input data.

-

-    y : array-like, shape (n_samples,)

-        Input data, target values.

-

-    pos_class : int, None

-        The class with respect to which we perform a one-vs-all fit.

-        If None, then it is assumed that the given problem is binary.

-

-    Cs : int | array-like, shape (n_cs,)

-        List of values for the regularization parameter or integer specifying

-        the number of regularization parameters that should be used. In this

-        case, the parameters will be chosen in a logarithmic scale between

-        1e-4 and 1e4.

-

-    fit_intercept : bool

-        Whether to fit an intercept for the model. In this case the shape of

-        the returned array is (n_cs, n_features + 1).

-

-    max_iter : int

-        Maximum number of iterations for the solver.

-

-    tol : float

-        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration

-        will stop when ``max{|g_i | i = 1, ..., n} <= tol``

-        where ``g_i`` is the i-th component of the gradient.

-

-    verbose : int

-        For the liblinear and lbfgs solvers set verbose to any positive

-        number for verbosity.

-

-    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'}

-        Numerical solver to use.

-

-    coef : array-like, shape (n_features,), default None

-        Initialization value for coefficients of logistic regression.

-        Useless for liblinear solver.

-

-    class_weight : dict or 'balanced', optional

-        Weights associated with classes in the form ``{class_label: weight}``.

-        If not given, all classes are supposed to have weight one.

-

-        The "balanced" mode uses the values of y to automatically adjust

-        weights inversely proportional to class frequencies in the input data

-        as ``n_samples / (n_classes * np.bincount(y))``.

-

-        Note that these weights will be multiplied with sample_weight (passed

-        through the fit method) if sample_weight is specified.

-

-    dual : bool

-        Dual or primal formulation. Dual formulation is only implemented for

-        l2 penalty with liblinear solver. Prefer dual=False when

-        n_samples > n_features.

-

-    penalty : str, 'l1' or 'l2'

-        Used to specify the norm used in the penalization. The 'newton-cg',

-        'sag' and 'lbfgs' solvers support only l2 penalties.

-

-    intercept_scaling : float, default 1.

-        Useful only when the solver 'liblinear' is used

-        and self.fit_intercept is set to True. In this case, x becomes

-        [x, self.intercept_scaling],

-        i.e. a "synthetic" feature with constant value equal to

-        intercept_scaling is appended to the instance vector.

-        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

-

-        Note! the synthetic feature weight is subject to l1/l2 regularization

-        as all other features.

-        To lessen the effect of regularization on synthetic feature weight

-        (and therefore on the intercept) intercept_scaling has to be increased.

-

-    multi_class : str, {'ovr', 'multinomial'}

-        Multiclass option can be either 'ovr' or 'multinomial'. If the option

-        chosen is 'ovr', then a binary problem is fit for each label. Else

-        the loss minimised is the multinomial loss fit across

-        the entire probability distribution. Does not work for 'liblinear'

-        solver.

-

-    random_state : int, RandomState instance or None, optional, default None

-        The seed of the pseudo random number generator to use when shuffling

-        the data.  If int, random_state is the seed used by the random number

-        generator; If RandomState instance, random_state is the random number

-        generator; If None, the random number generator is the RandomState

-        instance used by `np.random`. Used when ``solver`` == 'sag' or

-        'liblinear'.

-

-    check_input : bool, default True

-        If False, the input arrays X and y will not be checked.

-

-    max_squared_sum : float, default None

-        Maximum squared sum of X over samples. Used only in SAG solver.

-        If None, it will be computed, going through all the samples.

-        The value should be precomputed to speed up cross validation.

-

-    sample_weight : array-like, shape(n_samples,) optional

-        Array of weights that are assigned to individual samples.

-        If not provided, then each sample is given unit weight.

-

-    Returns

-    -------

-    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)

-        List of coefficients for the Logistic Regression model. If

-        fit_intercept is set to True then the second dimension will be

-        n_features + 1, where the last item represents the intercept.

-

-    Cs : ndarray

-        Grid of Cs used for cross-validation.

-

-    n_iter : array, shape (n_cs,)

-        Actual number of iteration for each Cs.

-

-    Notes

-    -----

-    You might get slightly different results with the solver liblinear than

-    with the others since this uses LIBLINEAR which penalizes the intercept.

-

-    .. versionchanged:: 0.19

-        The "copy" parameter was removed.

-    """

-    if isinstance(Cs, numbers.Integral):

-        Cs = np.logspace(-4, 4, Cs)

-

-    _check_solver_option(solver, multi_class, penalty, dual)

-

-    # Preprocessing.

-    if check_input:

-        X = check_array(X, accept_sparse='csr', dtype=np.float64)

-        y = check_array(y, ensure_2d=False, dtype=None)

-        check_consistent_length(X, y)

-    _, n_features = X.shape

-    classes = np.unique(y)

-    random_state = check_random_state(random_state)

-

-    if pos_class is None and multi_class != 'multinomial':

-        if (classes.size > 2):

-            raise ValueError('To fit OvR, use the pos_class argument')

-        # np.unique(y) gives labels in sorted order.

-        pos_class = classes[1]

-

-    # If sample weights exist, convert them to array (support for lists)

-    # and check length

-    # Otherwise set them to 1 for all examples

-    if sample_weight is not None:

-        sample_weight = np.array(sample_weight, dtype=X.dtype, order='C')

-        check_consistent_length(y, sample_weight)

-    else:

-        sample_weight = np.ones(X.shape[0], dtype=X.dtype)

-

-    # If class_weights is a dict (provided by the user), the weights

-    # are assigned to the original labels. If it is "balanced", then

-    # the class_weights are assigned after masking the labels with a OvR.

-    le = LabelEncoder()

-    if isinstance(class_weight, dict) or multi_class == 'multinomial':

-        class_weight_ = compute_class_weight(class_weight, classes, y)

-        sample_weight *= class_weight_[le.fit_transform(y)]

-

-    # For doing a ovr, we need to mask the labels first. for the

-    # multinomial case this is not necessary.

-    if multi_class == 'ovr':

-        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)

-        mask_classes = np.array([-1, 1])

-        mask = (y == pos_class)

-        y_bin = np.ones(y.shape, dtype=X.dtype)

-        y_bin[~mask] = -1.

-        # for compute_class_weight

-

-        if class_weight == "balanced":

-            class_weight_ = compute_class_weight(class_weight, mask_classes,

-                                                 y_bin)

-            sample_weight *= class_weight_[le.fit_transform(y_bin)]

-

-    else:

-        if solver not in ['sag', 'saga']:

-            lbin = LabelBinarizer()

-            Y_multi = lbin.fit_transform(y)

-            if Y_multi.shape[1] == 1:

-                Y_multi = np.hstack([1 - Y_multi, Y_multi])

+from sklearn.linear_model import RidgeClassifierCV

+

+class CustomRidgeClassifierCV(RidgeClassifierCV):

+    def __init__(self, alphas=np.arange(0.1, 1000, 0.1), normalize=True, 

+                 store_cv_values=False, **kwargs):

+        self.store_cv_values = store_cv_values

+        super().__init__(alphas=alphas, normalize=normalize, **kwargs)

+

+    def fit(self, X, y):

+        if self.store_cv_values:

+            return super().fit(X, y, store_cv_values=True)

         else:

-            # SAG multinomial solver needs LabelEncoder, not LabelBinarizer

-            le = LabelEncoder()

-            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)

-

-        w0 = np.zeros((classes.size, n_features + int(fit_intercept)),

-                      order='F', dtype=X.dtype)

-

-    if coef is not None:

-        # it must work both giving the bias term and not

-        if multi_class == 'ovr':

-            if coef.size not in (n_features, w0.size):

-                raise ValueError(

-                    'Initialization coef is of shape %d, expected shape '

-                    '%d or %d' % (coef.size, n_features, w0.size))

-            w0[:coef.size] = coef

-        else:

-            # For binary problems coef.shape[0] should be 1, otherwise it

-            # should be classes.size.

-            n_classes = classes.size

-            if n_classes == 2:

-                n_classes = 1

-

-            if (coef.shape[0] != n_classes or

-                    coef.shape[1] not in (n_features, n_features + 1)):

-                raise ValueError(

-                    'Initialization coef is of shape (%d, %d), expected '

-                    'shape (%d, %d) or (%d, %d)' % (

-                        coef.shape[0], coef.shape[1], classes.size,

-                        n_features, classes.size, n_features + 1))

-            w0[:, :coef.shape[1]] = coef

-

-    if multi_class == 'multinomial':

-        # fmin_l_bfgs_b and newton-cg accepts only ravelled parameters.

-        if solver in ['lbfgs', 'newton-cg']:

-            w0 = w0.ravel()

-        target = Y_multi

-        if solver == 'lbfgs':

-            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]

-        elif solver == 'newton-cg':

-            func = lambda x, *args: _multinomial_loss(x, *args)[0]

-            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]

-            hess = _multinomial_grad_hess

-        warm_start_sag = {'coef': w0.T}

-    else:

-        target = y_bin

-        if solver == 'lbfgs':

-            func = _logistic_loss_and_grad

-        elif solver == 'newton-cg':

-            func = _logistic_loss

-            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]

-            hess = _logistic_grad_hess

-        warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}

-

-    coefs = list()

-    n_iter = np.zeros(len(Cs), dtype=np.int32)

-    for i, C in enumerate(Cs):

-        if solver == 'lbfgs':

-            try:

-                w0, loss, info = optimize.fmin_l_bfgs_b(

-                    func, w0, fprime=None,

-                    args=(X, target, 1. / C, sample_weight),

-                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)

-            except TypeError:

-                # old scipy doesn't have maxiter

-                w0, loss, info = optimize.fmin_l_bfgs_b(

-                    func, w0, fprime=None,

-                    args=(X, target, 1. / C, sample_weight),

-                    iprint=(verbose > 0) - 1, pgtol=tol)

-            if info["warnflag"] == 1 and verbose > 0:

-                warnings.warn("lbfgs failed to converge. Increase the number "

-                              "of iterations.")

-            try:

-                n_iter_i = info['nit'] - 1

-            except:

-                n_iter_i = info['funcalls'] - 1

-        elif solver == 'newton-cg':

-            args = (X, target, 1. / C, sample_weight)

-            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,

-                                     maxiter=max_iter, tol=tol)

-        elif solver == 'liblinear':

-            coef_, intercept_, n_iter_i, = _fit_liblinear(

-                X, target, C, fit_intercept, intercept_scaling, None,

-                penalty, dual, verbose, max_iter, tol, random_state,

-                sample_weight=sample_weight)

-            if fit_intercept:

-                w0 = np.concatenate([coef_.ravel(), intercept_])

-            else:

-                w0 = coef_.ravel()

-

-        elif solver in ['sag', 'saga']:

-            if multi_class == 'multinomial':

-                target = target.astype(np.float64)

-                loss = 'multinomial'

-            else:

-                loss = 'log'

-            if penalty == 'l1':

-                alpha = 0.

-                beta = 1. / C

-            else:

-                alpha = 1. / C

-                beta = 0.

-            w0, n_iter_i, warm_start_sag = sag_solver(

-                X, target, sample_weight, loss, alpha,

-                beta, max_iter, tol,

-                verbose, random_state, False, max_squared_sum, warm_start_sag,

-                is_saga=(solver == 'saga'))

-

-        else:

-            raise ValueError("solver must be one of {'liblinear', 'lbfgs', "

-                             "'newton-cg', 'sag'}, got '%s' instead" % solver)

-

-        if multi_class == 'multinomial':

-            multi_w0 = np.reshape(w0, (classes.size, -1))

-            if classes.size == 2:

-                multi_w0 = multi_w0[1][np.newaxis, :]

-            coefs.append(multi_w0)

-        else:

-            coefs.append(w0.copy())

-

-        n_iter[i] = n_iter_i

-

-    return coefs, np.array(Cs), n_iter

+            return super().fit(X, y)

+

+# Test the function

+import numpy as np

+from sklearn import linear_model as lm

+

+# test database

+n = 100

+x = np.random.randn(n, 30)

+y = np.random.normal(size=n)

+

+rr = CustomRidgeClassifierCV(alphas=np.arange(0.1, 1000, 0.1), normalize=True, 

+                             store_cv_values=True).fit(x, y)

+

+print(rr.cv_values_)

 

 

 # helper function for LogisticCV


--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/theil_sen.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/theil_sen.py
@@ -331,6 +331,17 @@
 

         return n_subsamples, n_subpopulation

 

+from sklearn.linear_model import RidgeClassifierCV

+

+class CustomRidgeClassifierCV(RidgeClassifierCV):

+    def __init__(self, alphas=(1.0,), fit_intercept=True, normalize=False,

+                 scoring=None, cv=None, store_cv_values=False, class_weight=None,

+                 random_state=None):

+        super().__init__(alphas=alphas, fit_intercept=fit_intercept,

+                         normalize=normalize, scoring=scoring, cv=cv,

+                         class_weight=class_weight, random_state=random_state)

+        self.store_cv_values = store_cv_values

+

     def fit(self, X, y):

         """Fit linear model.

 

@@ -386,4 +397,7 @@
             self.intercept_ = 0.

             self.coef_ = coefs

 

+        if self.store_cv_values:

+            self.cv_values_ = np.zeros((n_samples, len(self.alphas)))

+

         return self


--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
@@ -771,13 +771,23 @@
 

 

 # helper function for LogisticCV

+def __init__(self, *args, **kwargs):

+        super().__init__(*args, **kwargs)

+        self.store_cv_values = kwargs.pop('store_cv_values', False)

+

+    def fit(self, X, y):

+        self.cv_values_ = None

+        self.store_cv_values_ = self.store_cv_values

+        return super().fit(X, y)

+

 def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,

                           scoring=None, fit_intercept=False,

                           max_iter=100, tol=1e-4, class_weight=None,

                           verbose=0, solver='lbfgs', penalty='l2',

                           dual=False, intercept_scaling=1.,

                           multi_class='ovr', random_state=None,

-                          max_squared_sum=None, sample_weight=None):

+                          max_squared_sum=None, sample_weight=None,

+                          store_cv_values=False):

     """Computes scores across logistic_regression_path

 

     Parameters

@@ -850,113 +860,7 @@
 

     intercept_scaling : float, default 1.

         Useful only when the solver 'liblinear' is used

-        and self.fit_intercept is set to True. In this case, x becomes

-        [x, self.intercept_scaling],

-        i.e. a "synthetic" feature with constant value equals to

-        intercept_scaling is appended to the instance vector.

-        The intercept becomes intercept_scaling * synthetic feature weight

-        Note! the synthetic feature weight is subject to l1/l2 regularization

-        as all other features.

-        To lessen the effect of regularization on synthetic feature weight

-        (and therefore on the intercept) intercept_scaling has to be increased.

-

-    multi_class : str, {'ovr', 'multinomial'}

-        Multiclass option can be either 'ovr' or 'multinomial'. If the option

-        chosen is 'ovr', then a binary problem is fit for each label. Else

-        the loss minimised is the multinomial loss fit across

-        the entire probability distribution. Does not work for 'liblinear'

-        solver.

-

-    random_state : int, RandomState instance or None, optional, default None

-        The seed of the pseudo random number generator to use when shuffling

-        the data.  If int, random_state is the seed used by the random number

-        generator; If RandomState instance, random_state is the random number

-        generator; If None, the random number generator is the RandomState

-        instance used by `np.random`. Used when ``solver`` == 'sag' and

-        'liblinear'.

-

-    max_squared_sum : float, default None

-        Maximum squared sum of X over samples. Used only in SAG solver.

-        If None, it will be computed, going through all the samples.

-        The value should be precomputed to speed up cross validation.

-

-    sample_weight : array-like, shape(n_samples,) optional

-        Array of weights that are assigned to individual samples.

-        If not provided, then each sample is given unit weight.

-

-    Returns

-    -------

-    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)

-        List of coefficients for the Logistic Regression model. If

-        fit_intercept is set to True then the second dimension will be

-        n_features + 1, where the last item represents the intercept.

-

-    Cs : ndarray

-        Grid of Cs used for cross-validation.

-

-    scores : ndarray, shape (n_cs,)

-        Scores obtained for each Cs.

-

-    n_iter : array, shape(n_cs,)

-        Actual number of iteration for each Cs.

-    """

-    _check_solver_option(solver, multi_class, penalty, dual)

-

-    X_train = X[train]

-    X_test = X[test]

-    y_train = y[train]

-    y_test = y[test]

-

-    if sample_weight is not None:

-        sample_weight = check_array(sample_weight, ensure_2d=False)

-        check_consistent_length(y, sample_weight)

-

-        sample_weight = sample_weight[train]

-

-    coefs, Cs, n_iter = logistic_regression_path(

-        X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,

-        solver=solver, max_iter=max_iter, class_weight=class_weight,

-        pos_class=pos_class, multi_class=multi_class,

-        tol=tol, verbose=verbose, dual=dual, penalty=penalty,

-        intercept_scaling=intercept_scaling, random_state=random_state,

-        check_input=False, max_squared_sum=max_squared_sum,

-        sample_weight=sample_weight)

-

-    log_reg = LogisticRegression(fit_intercept=fit_intercept)

-

-    # The score method of Logistic Regression has a classes_ attribute.

-    if multi_class == 'ovr':

-        log_reg.classes_ = np.array([-1, 1])

-    elif multi_class == 'multinomial':

-        log_reg.classes_ = np.unique(y_train)

-    else:

-        raise ValueError("multi_class should be either multinomial or ovr, "

-                         "got %d" % multi_class)

-

-    if pos_class is not None:

-        mask = (y_test == pos_class)

-        y_test = np.ones(y_test.shape, dtype=np.float64)

-        y_test[~mask] = -1.

-

-    scores = list()

-

-    if isinstance(scoring, six.string_types):

-        scoring = get_scorer(scoring)

-    for w in coefs:

-        if multi_class == 'ovr':

-            w = w[np.newaxis, :]

-        if fit_intercept:

-            log_reg.coef_ = w[:, :-1]

-            log_reg.intercept_ = w[:, -1]

-        else:

-            log_reg.coef_ = w

-            log_reg.intercept_ = 0.

-

-        if scoring is None:

-            scores.append(log_reg.score(X_test, y_test))

-        else:

-            scores.append(scoring(log_reg, X_test, y_test))

-    return coefs, Cs, np.array(scores), n_iter

+        and self.fit_intercept is set

 

 

 class LogisticRegression(BaseEstimator, LinearClassifierMixin,


