--- a/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
+++ b/decoupled/60/0.8_0.2/184/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
@@ -771,13 +771,23 @@
 

 

 # helper function for LogisticCV

+def __init__(self, *args, **kwargs):

+        super().__init__(*args, **kwargs)

+        self.store_cv_values = kwargs.pop('store_cv_values', False)

+

+    def fit(self, X, y):

+        self.cv_values_ = None

+        self.store_cv_values_ = self.store_cv_values

+        return super().fit(X, y)

+

 def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,

                           scoring=None, fit_intercept=False,

                           max_iter=100, tol=1e-4, class_weight=None,

                           verbose=0, solver='lbfgs', penalty='l2',

                           dual=False, intercept_scaling=1.,

                           multi_class='ovr', random_state=None,

-                          max_squared_sum=None, sample_weight=None):

+                          max_squared_sum=None, sample_weight=None,

+                          store_cv_values=False):

     """Computes scores across logistic_regression_path

 

     Parameters

@@ -850,113 +860,7 @@
 

     intercept_scaling : float, default 1.

         Useful only when the solver 'liblinear' is used

-        and self.fit_intercept is set to True. In this case, x becomes

-        [x, self.intercept_scaling],

-        i.e. a "synthetic" feature with constant value equals to

-        intercept_scaling is appended to the instance vector.

-        The intercept becomes intercept_scaling * synthetic feature weight

-        Note! the synthetic feature weight is subject to l1/l2 regularization

-        as all other features.

-        To lessen the effect of regularization on synthetic feature weight

-        (and therefore on the intercept) intercept_scaling has to be increased.

-

-    multi_class : str, {'ovr', 'multinomial'}

-        Multiclass option can be either 'ovr' or 'multinomial'. If the option

-        chosen is 'ovr', then a binary problem is fit for each label. Else

-        the loss minimised is the multinomial loss fit across

-        the entire probability distribution. Does not work for 'liblinear'

-        solver.

-

-    random_state : int, RandomState instance or None, optional, default None

-        The seed of the pseudo random number generator to use when shuffling

-        the data.  If int, random_state is the seed used by the random number

-        generator; If RandomState instance, random_state is the random number

-        generator; If None, the random number generator is the RandomState

-        instance used by `np.random`. Used when ``solver`` == 'sag' and

-        'liblinear'.

-

-    max_squared_sum : float, default None

-        Maximum squared sum of X over samples. Used only in SAG solver.

-        If None, it will be computed, going through all the samples.

-        The value should be precomputed to speed up cross validation.

-

-    sample_weight : array-like, shape(n_samples,) optional

-        Array of weights that are assigned to individual samples.

-        If not provided, then each sample is given unit weight.

-

-    Returns

-    -------

-    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)

-        List of coefficients for the Logistic Regression model. If

-        fit_intercept is set to True then the second dimension will be

-        n_features + 1, where the last item represents the intercept.

-

-    Cs : ndarray

-        Grid of Cs used for cross-validation.

-

-    scores : ndarray, shape (n_cs,)

-        Scores obtained for each Cs.

-

-    n_iter : array, shape(n_cs,)

-        Actual number of iteration for each Cs.

-    """

-    _check_solver_option(solver, multi_class, penalty, dual)

-

-    X_train = X[train]

-    X_test = X[test]

-    y_train = y[train]

-    y_test = y[test]

-

-    if sample_weight is not None:

-        sample_weight = check_array(sample_weight, ensure_2d=False)

-        check_consistent_length(y, sample_weight)

-

-        sample_weight = sample_weight[train]

-

-    coefs, Cs, n_iter = logistic_regression_path(

-        X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,

-        solver=solver, max_iter=max_iter, class_weight=class_weight,

-        pos_class=pos_class, multi_class=multi_class,

-        tol=tol, verbose=verbose, dual=dual, penalty=penalty,

-        intercept_scaling=intercept_scaling, random_state=random_state,

-        check_input=False, max_squared_sum=max_squared_sum,

-        sample_weight=sample_weight)

-

-    log_reg = LogisticRegression(fit_intercept=fit_intercept)

-

-    # The score method of Logistic Regression has a classes_ attribute.

-    if multi_class == 'ovr':

-        log_reg.classes_ = np.array([-1, 1])

-    elif multi_class == 'multinomial':

-        log_reg.classes_ = np.unique(y_train)

-    else:

-        raise ValueError("multi_class should be either multinomial or ovr, "

-                         "got %d" % multi_class)

-

-    if pos_class is not None:

-        mask = (y_test == pos_class)

-        y_test = np.ones(y_test.shape, dtype=np.float64)

-        y_test[~mask] = -1.

-

-    scores = list()

-

-    if isinstance(scoring, six.string_types):

-        scoring = get_scorer(scoring)

-    for w in coefs:

-        if multi_class == 'ovr':

-            w = w[np.newaxis, :]

-        if fit_intercept:

-            log_reg.coef_ = w[:, :-1]

-            log_reg.intercept_ = w[:, -1]

-        else:

-            log_reg.coef_ = w

-            log_reg.intercept_ = 0.

-

-        if scoring is None:

-            scores.append(log_reg.score(X_test, y_test))

-        else:

-            scores.append(scoring(log_reg, X_test, y_test))

-    return coefs, Cs, np.array(scores), n_iter

+        and self.fit_intercept is set

 

 

 class LogisticRegression(BaseEstimator, LinearClassifierMixin,
