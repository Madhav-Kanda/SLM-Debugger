--- a/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -730,87 +730,7 @@
 

     See also

     --------

-    cross_val_score : calculate score for each CV split

-

-    cross_validate : calculate one or more scores and timings for each CV split

-

-    Notes

-    -----

-    In the case that one or more classes are absent in a training portion, a

-    default score needs to be assigned to all instances for that class if

-    ``method`` produces columns per class, as in {'decision_function',

-    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is

-    0.  In order to ensure finite output, we approximate negative infinity by

-    the minimum finite float value for the dtype in other cases.

-

-    Examples

-    --------

-    >>> from sklearn import datasets, linear_model

-    >>> from sklearn.model_selection import cross_val_predict

-    >>> diabetes = datasets.load_diabetes()

-    >>> X = diabetes.data[:150]

-    >>> y = diabetes.target[:150]

-    >>> lasso = linear_model.Lasso()

-    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)

-    """

-    X, y, groups = indexable(X, y, groups)

-

-    cv = check_cv(cv, y, classifier=is_classifier(estimator))

-

-    # If classification methods produce multiple columns of output,

-    # we need to manually encode classes to ensure consistent column ordering.

-    encode = method in ['decision_function', 'predict_proba',

-                        'predict_log_proba']

-    if encode:

-        y = np.asarray(y)

-        if y.ndim == 1:

-            le = LabelEncoder()

-            y = le.fit_transform(y)

-        elif y.ndim == 2:

-            y_enc = np.zeros_like(y, dtype=np.int)

-            for i_label in range(y.shape[1]):

-                y_enc[:, i_label] = LabelEncoder().fit_transform(y[:, i_label])

-            y = y_enc

-

-    # We clone the estimator to make sure that all the folds are

-    # independent, and that it is pickle-able.

-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,

-                        pre_dispatch=pre_dispatch)

-    prediction_blocks = parallel(delayed(_fit_and_predict)(

-        clone(estimator), X, y, train, test, verbose, fit_params, method)

-        for train, test in cv.split(X, y, groups))

-

-    # Concatenate the predictions

-    predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]

-    test_indices = np.concatenate([indices_i

-                                   for _, indices_i in prediction_blocks])

-

-    if not _check_is_permutation(test_indices, _num_samples(X)):

-        raise ValueError('cross_val_predict only works for partitions')

-

-    inv_test_indices = np.empty(len(test_indices), dtype=int)

-    inv_test_indices[test_indices] = np.arange(len(test_indices))

-

-    if sp.issparse(predictions[0]):

-        predictions = sp.vstack(predictions, format=predictions[0].format)

-    elif encode and isinstance(predictions[0], list):

-        # `predictions` is a list of method outputs from each fold.

-        # If each of those is also a list, then treat this as a

-        # multioutput-multiclass task. We need to separately concatenate

-        # the method outputs for each label into an `n_labels` long list.

-        n_labels = y.shape[1]

-        concat_pred = []

-        for i_label in range(n_labels):

-            label_preds = np.concatenate([p[i_label] for p in predictions])

-            concat_pred.append(label_preds)

-        predictions = concat_pred

-    else:

-        predictions = np.concatenate(predictions)

-

-    if isinstance(predictions, list):

-        return [p[inv_test_indices] for p in predictions]

-    else:

-        return predictions[inv_test_indices]

+    cross_val_score :

 

 

 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params,


--- a/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -34,225 +34,18 @@
            'permutation_test_score', 'learning_curve', 'validation_curve']

 

 

-def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',

-                   n_jobs=None, verbose=0, fit_params=None,

-                   pre_dispatch='2*n_jobs', return_train_score=False,

-                   return_estimator=False, error_score='raise-deprecating'):

-    """Evaluate metric(s) by cross-validation and also record fit/score times.

-

-    Read more in the :ref:`User Guide <multimetric_cross_validation>`.

-

-    Parameters

-    ----------

-    estimator : estimator object implementing 'fit'

-        The object to use to fit the data.

-

-    X : array-like

-        The data to fit. Can be for example a list, or an array.

-

-    y : array-like, optional, default: None

-        The target variable to try to predict in the case of

-        supervised learning.

-

-    groups : array-like, with shape (n_samples,), optional

-        Group labels for the samples used while splitting the dataset into

-        train/test set.

-

-    scoring : string, callable, list/tuple, dict or None, default: None

-        A single string (see :ref:`scoring_parameter`) or a callable

-        (see :ref:`scoring`) to evaluate the predictions on the test set.

-

-        For evaluating multiple metrics, either give a list of (unique) strings

-        or a dict with names as keys and callables as values.

-

-        NOTE that when using custom scorers, each scorer should return a single

-        value. Metric functions returning a list/array of values can be wrapped

-        into multiple scorers that return one value each.

-

-        See :ref:`multimetric_grid_search` for an example.

-

-        If None, the estimator's score method is used.

-

-    cv : int, cross-validation generator or an iterable, optional

-        Determines the cross-validation splitting strategy.

-        Possible inputs for cv are:

-

-        - None, to use the default 3-fold cross validation,

-        - integer, to specify the number of folds in a `(Stratified)KFold`,

-        - :term:`CV splitter`,

-        - An iterable yielding (train, test) splits as arrays of indices.

-

-        For integer/None inputs, if the estimator is a classifier and ``y`` is

-        either binary or multiclass, :class:`StratifiedKFold` is used. In all

-        other cases, :class:`KFold` is used.

-

-        Refer :ref:`User Guide <cross_validation>` for the various

-        cross-validation strategies that can be used here.

-

-        .. versionchanged:: 0.20

-            ``cv`` default value if None will change from 3-fold to 5-fold

-            in v0.22.

-

-    n_jobs : int or None, optional (default=None)

-        The number of CPUs to use to do the computation.

-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.

-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`

-        for more details.

-

-    verbose : integer, optional

-        The verbosity level.

-

-    fit_params : dict, optional

-        Parameters to pass to the fit method of the estimator.

-

-    pre_dispatch : int, or string, optional

-        Controls the number of jobs that get dispatched during parallel

-        execution. Reducing this number can be useful to avoid an

-        explosion of memory consumption when more jobs get dispatched

-        than CPUs can process. This parameter can be:

-

-            - None, in which case all the jobs are immediately

-              created and spawned. Use this for lightweight and

-              fast-running jobs, to avoid delays due to on-demand

-              spawning of the jobs

-

-            - An int, giving the exact number of total jobs that are

-              spawned

-

-            - A string, giving an expression as a function of n_jobs,

-              as in '2*n_jobs'

-

-    return_train_score : boolean, default=False

-        Whether to include train scores.

-        Computing training scores is used to get insights on how different

-        parameter settings impact the overfitting/underfitting trade-off.

-        However computing the scores on the training set can be computationally

-        expensive and is not strictly required to select the parameters that

-        yield the best generalization performance.

-

-    return_estimator : boolean, default False

-        Whether to return the estimators fitted on each split.

-

-    error_score : 'raise' | 'raise-deprecating' or numeric

-        Value to assign to the score if an error occurs in estimator fitting.

-        If set to 'raise', the error is raised.

-        If set to 'raise-deprecating', a FutureWarning is printed before the

-        error is raised.

-        If a numeric value is given, FitFailedWarning is raised. This parameter

-        does not affect the refit step, which will always raise the error.

-        Default is 'raise-deprecating' but from version 0.22 it will change

-        to np.nan.

-

-    Returns

-    -------

-    scores : dict of float arrays of shape=(n_splits,)

-        Array of scores of the estimator for each run of the cross validation.

-

-        A dict of arrays containing the score/time arrays for each scorer is

-        returned. The possible keys for this ``dict`` are:

-

-            ``test_score``

-                The score array for test scores on each cv split.

-            ``train_score``

-                The score array for train scores on each cv split.

-                This is available only if ``return_train_score`` parameter

-                is ``True``.

-            ``fit_time``

-                The time for fitting the estimator on the train

-                set for each cv split.

-            ``score_time``

-                The time for scoring the estimator on the test set for each

-                cv split. (Note time for scoring on the train set is not

-                included even if ``return_train_score`` is set to ``True``

-            ``estimator``

-                The estimator objects for each cv split.

-                This is available only if ``return_estimator`` parameter

-                is set to ``True``.

-

-    Examples

-    --------

-    >>> from sklearn import datasets, linear_model

-    >>> from sklearn.model_selection import cross_validate

-    >>> from sklearn.metrics.scorer import make_scorer

-    >>> from sklearn.metrics import confusion_matrix

-    >>> from sklearn.svm import LinearSVC

-    >>> diabetes = datasets.load_diabetes()

-    >>> X = diabetes.data[:150]

-    >>> y = diabetes.target[:150]

-    >>> lasso = linear_model.Lasso()

-

-    Single metric evaluation using ``cross_validate``

-

-    >>> cv_results = cross_validate(lasso, X, y, cv=3)

-    >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS

-    ['fit_time', 'score_time', 'test_score']

-    >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE

-    array([0.33150734, 0.08022311, 0.03531764])

-

-    Multiple metric evaluation using ``cross_validate``

-    (please refer the ``scoring`` parameter doc for more information)

-

-    >>> scores = cross_validate(lasso, X, y, cv=3,

-    ...                         scoring=('r2', 'neg_mean_squared_error'),

-    ...                         return_train_score=True)

-    >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS

-    [-3635.5... -3573.3... -6114.7...]

-    >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS

-    [0.28010158 0.39088426 0.22784852]

-

-    See Also

-    ---------

-    :func:`sklearn.model_selection.cross_val_score`:

-        Run cross-validation for single metric evaluation.

-

-    :func:`sklearn.model_selection.cross_val_predict`:

-        Get predictions from each split of cross-validation for diagnostic

-        purposes.

-

-    :func:`sklearn.metrics.make_scorer`:

-        Make a scorer from a performance metric or loss function.

-

-    """

-    X, y, groups = indexable(X, y, groups)

-

-    cv = check_cv(cv, y, classifier=is_classifier(estimator))

-    scorers, _ = _check_multimetric_scoring(estimator, scoring=scoring)

-

-    # We clone the estimator to make sure that all the folds are

-    # independent, and that it is pickle-able.

-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,

-                        pre_dispatch=pre_dispatch)

-    scores = parallel(

-        delayed(_fit_and_score)(

-            clone(estimator), X, y, scorers, train, test, verbose, None,

-            fit_params, return_train_score=return_train_score,

-            return_times=True, return_estimator=return_estimator,

-            error_score=error_score)

-        for train, test in cv.split(X, y, groups))

-

-    zipped_scores = list(zip(*scores))

-    if return_train_score:

-        train_scores = zipped_scores.pop(0)

-        train_scores = _aggregate_score_dicts(train_scores)

-    if return_estimator:

-        fitted_estimators = zipped_scores.pop()

-    test_scores, fit_times, score_times = zipped_scores

-    test_scores = _aggregate_score_dicts(test_scores)

-

-    ret = {}

-    ret['fit_time'] = np.array(fit_times)

-    ret['score_time'] = np.array(score_times)

-

-    if return_estimator:

-        ret['estimator'] = fitted_estimators

-

-    for name in scorers:

-        ret['test_%s' % name] = np.array(test_scores[name])

-        if return_train_score:

-            key = 'train_%s' % name

-            ret[key] = np.array(train_scores[name])

-

-    return ret

+import sklearn

+import numpy as np

+from sklearn.linear_model import LogisticRegressionCV

+

+# Set the configuration to print only changed parameters

+sklearn.set_config(print_changed_only=True)

+

+# Ensure the Cs parameter is always a list or an array

+Cs = np.array([0.1, 1])

+

+# Print the LogisticRegressionCV object

+print(LogisticRegressionCV(Cs=Cs))

 

 

 def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',


--- a/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -255,139 +255,25 @@
     return ret

 

 

-def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',

-                    n_jobs=None, verbose=0, fit_params=None,

-                    pre_dispatch='2*n_jobs', error_score='raise-deprecating'):

-    """Evaluate a score by cross-validation

-

-    Read more in the :ref:`User Guide <cross_validation>`.

-

-    Parameters

-    ----------

-    estimator : estimator object implementing 'fit'

-        The object to use to fit the data.

-

-    X : array-like

-        The data to fit. Can be for example a list, or an array.

-

-    y : array-like, optional, default: None

-        The target variable to try to predict in the case of

-        supervised learning.

-

-    groups : array-like, with shape (n_samples,), optional

-        Group labels for the samples used while splitting the dataset into

-        train/test set.

-

-    scoring : string, callable or None, optional, default: None

-        A string (see model evaluation documentation) or

-        a scorer callable object / function with signature

-        ``scorer(estimator, X, y)`` which should return only

-        a single value.

-

-        Similar to :func:`cross_validate`

-        but only a single metric is permitted.

-

-        If None, the estimator's default scorer (if available) is used.

-

-    cv : int, cross-validation generator or an iterable, optional

-        Determines the cross-validation splitting strategy.

-        Possible inputs for cv are:

-

-        - None, to use the default 3-fold cross validation,

-        - integer, to specify the number of folds in a `(Stratified)KFold`,

-        - :term:`CV splitter`,

-        - An iterable yielding (train, test) splits as arrays of indices.

-

-        For integer/None inputs, if the estimator is a classifier and ``y`` is

-        either binary or multiclass, :class:`StratifiedKFold` is used. In all

-        other cases, :class:`KFold` is used.

-

-        Refer :ref:`User Guide <cross_validation>` for the various

-        cross-validation strategies that can be used here.

-

-        .. versionchanged:: 0.20

-            ``cv`` default value if None will change from 3-fold to 5-fold

-            in v0.22.

-

-    n_jobs : int or None, optional (default=None)

-        The number of CPUs to use to do the computation.

-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.

-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`

-        for more details.

-

-    verbose : integer, optional

-        The verbosity level.

-

-    fit_params : dict, optional

-        Parameters to pass to the fit method of the estimator.

-

-    pre_dispatch : int, or string, optional

-        Controls the number of jobs that get dispatched during parallel

-        execution. Reducing this number can be useful to avoid an

-        explosion of memory consumption when more jobs get dispatched

-        than CPUs can process. This parameter can be:

-

-            - None, in which case all the jobs are immediately

-              created and spawned. Use this for lightweight and

-              fast-running jobs, to avoid delays due to on-demand

-              spawning of the jobs

-

-            - An int, giving the exact number of total jobs that are

-              spawned

-

-            - A string, giving an expression as a function of n_jobs,

-              as in '2*n_jobs'

-

-    error_score : 'raise' | 'raise-deprecating' or numeric

-        Value to assign to the score if an error occurs in estimator fitting.

-        If set to 'raise', the error is raised.

-        If set to 'raise-deprecating', a FutureWarning is printed before the

-        error is raised.

-        If a numeric value is given, FitFailedWarning is raised. This parameter

-        does not affect the refit step, which will always raise the error.

-        Default is 'raise-deprecating' but from version 0.22 it will change

-        to np.nan.

-

-    Returns

-    -------

-    scores : array of float, shape=(len(list(cv)),)

-        Array of scores of the estimator for each run of the cross validation.

-

-    Examples

-    --------

-    >>> from sklearn import datasets, linear_model

-    >>> from sklearn.model_selection import cross_val_score

-    >>> diabetes = datasets.load_diabetes()

-    >>> X = diabetes.data[:150]

-    >>> y = diabetes.target[:150]

-    >>> lasso = linear_model.Lasso()

-    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS

-    [0.33150734 0.08022311 0.03531764]

-

-    See Also

-    ---------

-    :func:`sklearn.model_selection.cross_validate`:

-        To run cross-validation on multiple metrics and also to return

-        train scores, fit times and score times.

-

-    :func:`sklearn.model_selection.cross_val_predict`:

-        Get predictions from each split of cross-validation for diagnostic

-        purposes.

-

-    :func:`sklearn.metrics.make_scorer`:

-        Make a scorer from a performance metric or loss function.

-

-    """

-    # To ensure multimetric format is not supported

-    scorer = check_scoring(estimator, scoring=scoring)

-

-    cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,

-                                scoring={'score': scorer}, cv=cv,

-                                n_jobs=n_jobs, verbose=verbose,

-                                fit_params=fit_params,

-                                pre_dispatch=pre_dispatch,

-                                error_score=error_score)

-    return cv_results['test_score']

+import sklearn

+import numpy as np

+from sklearn.linear_model import LogisticRegressionCV

+

+def print_changed_only_config():

+    # Save the current configuration

+    original_config = sklearn.get_config()

+    

+    # Disable print_changed_only

+    sklearn.set_config(print_changed_only=False)

+    

+    # Print the LogisticRegressionCV object

+    print(LogisticRegressionCV(Cs=np.array([0.1, 1])))

+    

+    # Restore the original configuration

+    sklearn.set_config(**original_config)

+

+# Call the function to print the LogisticRegressionCV object

+print_changed_only_config()

 

 

 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,


--- a/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -104,8 +104,7 @@
     XT : {array, sparse matrix}

         An array of type np.float

     """

-    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)

-                                    and not sp.issparse(X)):

+    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray) and not sp.issparse(X)):

         return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,

                            copy=copy, force_all_finite=force_all_finite,

                            ensure_2d=False)


--- a/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/195/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -40,7 +40,9 @@
 

     if _get_config()['assume_finite']:

         return

+

     X = np.asanyarray(X)

+

     # First try an O(n) time, O(1) space solution for the common case that

     # everything is finite; fall back to O(n) space np.isfinite to prevent

     # false positives from overflow in sum method. The sum is also calculated


