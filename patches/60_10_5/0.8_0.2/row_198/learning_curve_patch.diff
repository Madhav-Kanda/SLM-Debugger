--- a/decoupled/60/0.8_0.2/198/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/198/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -1159,109 +1159,7 @@
         a scorer callable object / function with signature

         ``scorer(estimator, X, y)``.

 

-    exploit_incremental_learning : boolean, optional, default: False

-        If the estimator supports incremental learning, this will be

-        used to speed up fitting for different training set sizes.

-

-    n_jobs : int or None, optional (default=None)

-        Number of jobs to run in parallel.

-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.

-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`

-        for more details.

-

-    pre_dispatch : integer or string, optional

-        Number of predispatched jobs for parallel execution (default is

-        all). The option can reduce the allocated memory. The string can

-        be an expression like '2*n_jobs'.

-

-    verbose : integer, optional

-        Controls the verbosity: the higher, the more messages.

-

-    shuffle : boolean, optional

-        Whether to shuffle training data before taking prefixes of it

-        based on``train_sizes``.

-

-    random_state : int, RandomState instance or None, optional (default=None)

-        If int, random_state is the seed used by the random number generator;

-        If RandomState instance, random_state is the random number generator;

-        If None, the random number generator is the RandomState instance used

-        by `np.random`. Used when ``shuffle`` is True.

-

-    error_score : 'raise' or numeric

-        Value to assign to the score if an error occurs in estimator fitting.

-        If set to 'raise', the error is raised.

-        If a numeric value is given, FitFailedWarning is raised. This parameter

-        does not affect the refit step, which will always raise the error.

-

-    Returns

-    -------

-    train_sizes_abs : array, shape (n_unique_ticks,), dtype int

-        Numbers of training examples that has been used to generate the

-        learning curve. Note that the number of ticks might be less

-        than n_ticks because duplicate entries will be removed.

-

-    train_scores : array, shape (n_ticks, n_cv_folds)

-        Scores on training sets.

-

-    test_scores : array, shape (n_ticks, n_cv_folds)

-        Scores on test set.

-

-    Notes

-    -----

-    See :ref:`examples/model_selection/plot_learning_curve.py

-    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`

-    """

-    if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):

-        raise ValueError("An estimator must support the partial_fit interface "

-                         "to exploit incremental learning")

-    X, y, groups = indexable(X, y, groups)

-

-    cv = check_cv(cv, y, classifier=is_classifier(estimator))

-    # Store it as list as we will be iterating over the list multiple times

-    cv_iter = list(cv.split(X, y, groups))

-

-    scorer = check_scoring(estimator, scoring=scoring)

-

-    n_max_training_samples = len(cv_iter[0][0])

-    # Because the lengths of folds can be significantly different, it is

-    # not guaranteed that we use all of the available training data when we

-    # use the first 'n_max_training_samples' samples.

-    train_sizes_abs = _translate_train_sizes(train_sizes,

-                                             n_max_training_samples)

-    n_unique_ticks = train_sizes_abs.shape[0]

-    if verbose > 0:

-        print("[learning_curve] Training set sizes: " + str(train_sizes_abs))

-

-    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,

-                        verbose=verbose)

-

-    if shuffle:

-        rng = check_random_state(random_state)

-        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)

-

-    if exploit_incremental_learning:

-        classes = np.unique(y) if is_classifier(estimator) else None

-        out = parallel(delayed(_incremental_fit_estimator)(

-            clone(estimator), X, y, classes, train, test, train_sizes_abs,

-            scorer, verbose) for train, test in cv_iter)

-    else:

-        train_test_proportions = []

-        for train, test in cv_iter:

-            for n_train_samples in train_sizes_abs:

-                train_test_proportions.append((train[:n_train_samples], test))

-

-        out = parallel(delayed(_fit_and_score)(

-            clone(estimator), X, y, scorer, train, test, verbose,

-            parameters=None, fit_params=None, return_train_score=True,

-            error_score=error_score)

-            for train, test in train_test_proportions)

-        out = np.array(out)

-        n_cv_folds = out.shape[0] // n_unique_ticks

-        out = out.reshape(n_cv_folds, n_unique_ticks, 2)

-

-    out = np.asarray(out).transpose((2, 1, 0))

-

-    return train_sizes_abs, out[0], out[1]

+    exploit_increment

 

 

 def _translate_train_sizes(train_sizes, n_max_training_samples):
