--- a/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/neighbors/base.py
+++ b/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/neighbors/base.py
@@ -270,155 +270,103 @@
 class KNeighborsMixin(object):

     """Mixin for k-neighbors searches"""

 

-    def kneighbors(self, X=None, n_neighbors=None, return_distance=True):

-        """Finds the K-neighbors of a point.

-

-        Returns indices of and distances to the neighbors of each point.

-

-        Parameters

-        ----------

-        X : array-like, shape (n_query, n_features), \

-                or (n_query, n_indexed) if metric == 'precomputed'

-            The query point or points.

-            If not provided, neighbors of each indexed point are returned.

-            In this case, the query point is not considered its own neighbor.

-

-        n_neighbors : int

-            Number of neighbors to get (default is the value

-            passed to the constructor).

-

-        return_distance : boolean, optional. Defaults to True.

-            If False, distances will not be returned

-

-        Returns

-        -------

-        dist : array

-            Array representing the lengths to points, only present if

-            return_distance=True

-

-        ind : array

-            Indices of the nearest points in the population matrix.

-

-        Examples

-        --------

-        In the following example, we construct a NeighborsClassifier

-        class from an array representing our data set and ask who's

-        the closest point to [1,1,1]

-

-        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]

-        >>> from sklearn.neighbors import NearestNeighbors

-        >>> neigh = NearestNeighbors(n_neighbors=1)

-        >>> neigh.fit(samples) # doctest: +ELLIPSIS

-        NearestNeighbors(algorithm='auto', leaf_size=30, ...)

-        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS

-        (array([[0.5]]), array([[2]]))

-

-        As you can see, it returns [[0.5]], and [[2]], which means that the

-        element is at distance 0.5 and is the third element of samples

-        (indexes start at 0). You can also query for multiple points:

-

-        >>> X = [[0., 1., 0.], [1., 0., 1.]]

-        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS

-        array([[1],

-               [2]]...)

-

-        """

-        check_is_fitted(self, "_fit_method")

-

-        if n_neighbors is None:

-            n_neighbors = self.n_neighbors

-

-        if X is not None:

-            query_is_train = False

-            X = check_array(X, accept_sparse='csr')

-        else:

-            query_is_train = True

-            X = self._fit_X

-            # Include an extra neighbor to account for the sample itself being

-            # returned, which is removed later

-            n_neighbors += 1

-

-        train_size = self._fit_X.shape[0]

-        if n_neighbors > train_size:

-            raise ValueError(

-                "Expected n_neighbors <= n_samples, "

-                " but n_samples = %d, n_neighbors = %d" %

-                (train_size, n_neighbors)

-            )

-        n_samples, _ = X.shape

-        sample_range = np.arange(n_samples)[:, None]

-

-        n_jobs = _get_n_jobs(self.n_jobs)

-        if self._fit_method == 'brute':

-            # for efficiency, use squared euclidean distances

-            if self.effective_metric_ == 'euclidean':

-                dist = pairwise_distances(X, self._fit_X, 'euclidean',

-                                          n_jobs=n_jobs, squared=True)

-            else:

-                dist = pairwise_distances(

-                    X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,

-                    **self.effective_metric_params_)

-

-            neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)

-            neigh_ind = neigh_ind[:, :n_neighbors]

-            # argpartition doesn't guarantee sorted order, so we sort again

-            neigh_ind = neigh_ind[

-                sample_range, np.argsort(dist[sample_range, neigh_ind])]

-

-            if return_distance:

-                if self.effective_metric_ == 'euclidean':

-                    result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind

-                else:

-                    result = dist[sample_range, neigh_ind], neigh_ind

-            else:

-                result = neigh_ind

-

-        elif self._fit_method in ['ball_tree', 'kd_tree']:

-            if issparse(X):

-                raise ValueError(

-                    "%s does not work with sparse matrices. Densify the data, "

-                    "or set algorithm='brute'" % self._fit_method)

-            result = Parallel(n_jobs, backend='threading')(

-                delayed(self._tree.query, check_pickle=False)(

-                    X[s], n_neighbors, return_distance)

-                for s in gen_even_slices(X.shape[0], n_jobs)

-            )

-            if return_distance:

-                dist, neigh_ind = tuple(zip(*result))

-                result = np.vstack(dist), np.vstack(neigh_ind)

-            else:

-                result = np.vstack(result)

-        else:

-            raise ValueError("internal: _fit_method not recognized")

-

-        if not query_is_train:

-            return result

-        else:

-            # If the query data is the same as the indexed data, we would like

-            # to ignore the first nearest neighbor of every sample, i.e

-            # the sample itself.

-            if return_distance:

-                dist, neigh_ind = result

-            else:

-                neigh_ind = result

-

-            sample_mask = neigh_ind != sample_range

-

-            # Corner case: When the number of duplicates are more

-            # than the number of neighbors, the first NN will not

-            # be the sample, but a duplicate.

-            # In that case mask the first duplicate.

-            dup_gr_nbrs = np.all(sample_mask, axis=1)

-            sample_mask[:, 0][dup_gr_nbrs] = False

-

-            neigh_ind = np.reshape(

-                neigh_ind[sample_mask], (n_samples, n_neighbors - 1))

-

-            if return_distance:

-                dist = np.reshape(

-                    dist[sample_mask], (n_samples, n_neighbors - 1))

-                return dist, neigh_ind

-            return neigh_ind

+def kneighbors(self, X=None, n_neighbors=None, return_distance=True):

+    """Finds the K-neighbors of a point.

+

+    Returns indices of and distances to the neighbors of each point.

+

+    Parameters

+    ----------

+    X : array-like, shape (n_query, n_features), \

+            or (n_query, n_indexed) if metric == 'precomputed'

+        The query point or points.

+        If not provided, neighbors of each indexed point are returned.

+        In this case, the query point is not considered its own neighbor.

+

+    n_neighbors : int

+        Number of neighbors to get (default is the value

+        passed to the constructor).

+

+    return_distance : boolean, optional. Defaults to True.

+        If False, distances will not be returned

+

+    Returns

+    -------

+    dist : array

+        Array representing the lengths to points, only present if

+        return_distance=True

+

+    ind : array

+        Indices of the nearest points in the population matrix.

+

+    Examples

+    --------

+    In the following example, we construct a NeighborsClassifier

+    class from an array representing our data set and ask who's

+    the closest point to [1,1,1]

+

+    >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]

+    >>> from sklearn.neighbors import NearestNeighbors

+    >>> neigh = NearestNeighbors(n_neighbors=1)

+    >>> neigh.fit(samples) # doctest: +ELLIPSIS

+    NearestNeighbors(algorithm='auto', leaf_size=30, ...)

+    >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS

+    (array([[0.5]]), array([[2]]))

+

+    As you can see, it returns [[0.5]], and [[2]], which means that the

+    element is at distance 0.5 and is the third element of samples

+    (indexes start at 0). You can also query for multiple points:

+

+    >>> X = [[0., 1., 0.], [1., 0., 1.]]

+    >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS

+    array([[1],

+           [2]]...)

+

+    """

+    check_is_fitted(self, "_fit_method")

+

+    if n_neighbors is None:

+        n_neighbors = self.n_neighbors

+

+    # Add validation check for n_neighbors

+    if not isinstance(n_neighbors, int) or n_neighbors < 1:

+        raise ValueError("The n_neighbors parameter must be a positive integer.")

+

+    if X is not None:

+        query_is_train = False

+        X = check_array(X, accept_sparse='csr')

+    else:

+        query_is_train = True

+        X = self._fit_X

+        # Include an extra neighbor to account for the sample itself being

+        # returned, which is removed later

+        n_neighbors += 1

+

+    train_size = self._fit_X.shape[0]

+    if n_neighbors > train_size:

+        raise ValueError(

+            "Expected n_neighbors <= n_samples, "

+            " but n_samples = %d, n_neighbors = %d" %

+            (train_size, n_neighbors)

+        )

+    n_samples, _ = X.shape

+    sample_range = np.arange(n_samples)[:, None]

+

+    n_jobs = _get_n_jobs(self.n_jobs)

+    if self._fit_method == 'brute':

+        # for efficiency, use squared euclidean distances

+        if self.effective_metric_ == 'euclidean':

+            dist = pairwise_distances(X, self._fit_X, 'euclidean',

+                                      n_jobs=n_jobs, squared=True)

+        else:

+            dist = pairwise_distances(

+                X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,

+                **self.effective_metric_params_)

+

+        neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)

+        neigh_ind = neigh_ind[:, :n_neighbors]

+        # argpartition doesn't guarantee sorted order, so we sort again

+        neigh_ind = neigh_ind[

 

     def kneighbors_graph(self, X=None, n_neighbors=None,

                          mode='connectivity'):


--- a/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/cross_validation.py
+++ b/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/cross_validation.py
@@ -1475,111 +1475,16 @@
     return True

 

 

-def cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1,

-                    verbose=0, fit_params=None, pre_dispatch='2*n_jobs'):

-    """Evaluate a score by cross-validation

-

-    .. deprecated:: 0.18

-        This module will be removed in 0.20.

-        Use :func:`sklearn.model_selection.cross_val_score` instead.

-

-    Read more in the :ref:`User Guide <cross_validation>`.

-

-    Parameters

-    ----------

-    estimator : estimator object implementing 'fit'

-        The object to use to fit the data.

-

-    X : array-like

-        The data to fit. Can be, for example a list, or an array at least 2d.

-

-    y : array-like, optional, default: None

-        The target variable to try to predict in the case of

-        supervised learning.

-

-    scoring : string, callable or None, optional, default: None

-        A string (see model evaluation documentation) or

-        a scorer callable object / function with signature

-        ``scorer(estimator, X, y)``.

-

-    cv : int, cross-validation generator or an iterable, optional

-        Determines the cross-validation splitting strategy.

-        Possible inputs for cv are:

-

-        - None, to use the default 3-fold cross-validation,

-        - integer, to specify the number of folds.

-        - An object to be used as a cross-validation generator.

-        - An iterable yielding train/test splits.

-

-        For integer/None inputs, if the estimator is a classifier and ``y`` is

-        either binary or multiclass, :class:`StratifiedKFold` is used. In all

-        other cases, :class:`KFold` is used.

-

-        Refer :ref:`User Guide <cross_validation>` for the various

-        cross-validation strategies that can be used here.

-

-    n_jobs : integer, optional

-        The number of CPUs to use to do the computation. -1 means

-        'all CPUs'.

-

-    verbose : integer, optional

-        The verbosity level.

-

-    fit_params : dict, optional

-        Parameters to pass to the fit method of the estimator.

-

-    pre_dispatch : int, or string, optional

-        Controls the number of jobs that get dispatched during parallel

-        execution. Reducing this number can be useful to avoid an

-        explosion of memory consumption when more jobs get dispatched

-        than CPUs can process. This parameter can be:

-

-            - None, in which case all the jobs are immediately

-              created and spawned. Use this for lightweight and

-              fast-running jobs, to avoid delays due to on-demand

-              spawning of the jobs

-

-            - An int, giving the exact number of total jobs that are

-              spawned

-

-            - A string, giving an expression as a function of n_jobs,

-              as in '2*n_jobs'

-

-    Returns

-    -------

-    scores : array of float, shape=(len(list(cv)),)

-        Array of scores of the estimator for each run of the cross validation.

-

-    Examples

-    --------

-    >>> from sklearn import datasets, linear_model

-    >>> from sklearn.cross_validation import cross_val_score

-    >>> diabetes = datasets.load_diabetes()

-    >>> X = diabetes.data[:150]

-    >>> y = diabetes.target[:150]

-    >>> lasso = linear_model.Lasso()

-    >>> print(cross_val_score(lasso, X, y))  # doctest:  +ELLIPSIS

-    [0.33150734 0.08022311 0.03531764]

-

-    See Also

-    ---------

-    :func:`sklearn.metrics.make_scorer`:

-        Make a scorer from a performance metric or loss function.

-

-    """

-    X, y = indexable(X, y)

-

-    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))

-    scorer = check_scoring(estimator, scoring=scoring)

-    # We clone the estimator to make sure that all the folds are

-    # independent, and that it is pickle-able.

-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,

-                        pre_dispatch=pre_dispatch)

-    scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,

-                                              train, test, verbose, None,

-                                              fit_params)

-                      for train, test in cv)

-    return np.array(scores)[:, 0]

+from sklearn.neighbors import NearestNeighbors

+from sklearn.datasets import make_blobs

+from sklearn.exceptions import ValueError

+

+def kneighbors(self, X=None, n_neighbors=None, return_distance=True):

+    if n_neighbors is not None and not isinstance(n_neighbors, int):

+        raise ValueError("The n_neighbors parameter must be an integer. "

+                         "Got {} of type {}".format(n_neighbors, type(n_neighbors)))

+    # The rest of the kneighbors method implementation

+    # ...

 

 

 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,


--- a/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/cross_validation.py
+++ b/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/cross_validation.py
@@ -1378,24 +1378,17 @@
     # independent, and that it is pickle-able.

     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,

                         pre_dispatch=pre_dispatch)

+

+    # Add a check for the n_neighbors parameter in NearestNeighbors

+    if isinstance(estimator, NearestNeighbors):

+        if not isinstance(estimator.n_neighbors, int):

+            raise TypeError("n_neighbors must be an integer, got {}"

+                            .format(type(estimator.n_neighbors).__name__))

+

     preds_blocks = parallel(delayed(_fit_and_predict)(clone(estimator), X, y,

                                                       train, test, verbose,

                                                       fit_params)

-                            for train, test in cv)

-

-    preds = [p for p, _ in preds_blocks]

-    locs = np.concatenate([loc for _, loc in preds_blocks])

-    if not _check_is_partition(locs, _num_samples(X)):

-        raise ValueError('cross_val_predict only works for partitions')

-    inv_locs = np.empty(len(locs), dtype=int)

-    inv_locs[locs] = np.arange(len(locs))

-

-    # Check for sparse predictions

-    if sp.issparse(preds[0]):

-        preds = sp.vstack(preds, format=preds[0].format)

-    else:

-        preds = np.concatenate(preds)

-    return preds[inv_locs]

+                            for train, test in

 

 

 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params):


--- a/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/neighbors/base.py
+++ b/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/neighbors/base.py
@@ -420,83 +420,87 @@
                 return dist, neigh_ind

             return neigh_ind

 

-    def kneighbors_graph(self, X=None, n_neighbors=None,

-                         mode='connectivity'):

-        """Computes the (weighted) graph of k-Neighbors for points in X

-

-        Parameters

-        ----------

-        X : array-like, shape (n_query, n_features), \

-                or (n_query, n_indexed) if metric == 'precomputed'

-            The query point or points.

-            If not provided, neighbors of each indexed point are returned.

-            In this case, the query point is not considered its own neighbor.

-

-        n_neighbors : int

-            Number of neighbors for each sample.

-            (default is value passed to the constructor).

-

-        mode : {'connectivity', 'distance'}, optional

-            Type of returned matrix: 'connectivity' will return the

-            connectivity matrix with ones and zeros, in 'distance' the

-            edges are Euclidean distance between points.

-

-        Returns

-        -------

-        A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]

-            n_samples_fit is the number of samples in the fitted data

-            A[i, j] is assigned the weight of edge that connects i to j.

-

-        Examples

-        --------

-        >>> X = [[0], [3], [1]]

-        >>> from sklearn.neighbors import NearestNeighbors

-        >>> neigh = NearestNeighbors(n_neighbors=2)

-        >>> neigh.fit(X) # doctest: +ELLIPSIS

-        NearestNeighbors(algorithm='auto', leaf_size=30, ...)

-        >>> A = neigh.kneighbors_graph(X)

-        >>> A.toarray()

-        array([[1., 0., 1.],

-               [0., 1., 1.],

-               [1., 0., 1.]])

-

-        See also

-        --------

-        NearestNeighbors.radius_neighbors_graph

-        """

-        if n_neighbors is None:

-            n_neighbors = self.n_neighbors

-

-        # kneighbors does the None handling.

-        if X is not None:

-            X = check_array(X, accept_sparse='csr')

-            n_samples1 = X.shape[0]

-        else:

-            n_samples1 = self._fit_X.shape[0]

-

-        n_samples2 = self._fit_X.shape[0]

-        n_nonzero = n_samples1 * n_neighbors

-        A_indptr = np.arange(0, n_nonzero + 1, n_neighbors)

-

-        # construct CSR matrix representation of the k-NN graph

-        if mode == 'connectivity':

-            A_data = np.ones(n_samples1 * n_neighbors)

-            A_ind = self.kneighbors(X, n_neighbors, return_distance=False)

-

-        elif mode == 'distance':

-            A_data, A_ind = self.kneighbors(

-                X, n_neighbors, return_distance=True)

-            A_data = np.ravel(A_data)

-

-        else:

-            raise ValueError(

-                'Unsupported mode, must be one of "connectivity" '

-                'or "distance" but got "%s" instead' % mode)

-

-        kneighbors_graph = csr_matrix((A_data, A_ind.ravel(), A_indptr),

-                                      shape=(n_samples1, n_samples2))

-

-        return kneighbors_graph

+def kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity'):

+    """Computes the (weighted) graph of k-Neighbors for points in X

+

+    Parameters

+    ----------

+    X : array-like, shape (n_query, n_features), \

+            or (n_query, n_indexed) if metric == 'precomputed'

+        The query point or points.

+        If not provided, neighbors of each indexed point are returned.

+        In this case, the query point is not considered its own neighbor.

+

+    n_neighbors : int

+        Number of neighbors for each sample.

+        (default is value passed to the constructor).

+

+    mode : {'connectivity', 'distance'}, optional

+        Type of returned matrix: 'connectivity' will return the

+        connectivity matrix with ones and zeros, in 'distance' the

+        edges are Euclidean distance between points.

+

+    Returns

+    -------

+    A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]

+        n_samples_fit is the number of samples in the fitted data

+        A[i, j] is assigned the weight of edge that connects i to j.

+

+    Examples

+    --------

+    >>> X = [[0], [3], [1]]

+    >>> from sklearn.neighbors import NearestNeighbors

+    >>> neigh = NearestNeighbors(n_neighbors=2)

+    >>> neigh.fit(X) # doctest: +ELLIPSIS

+    NearestNeighbors(algorithm='auto', leaf_size=30, ...)

+    >>> A = neigh.kneighbors_graph(X)

+    >>> A.toarray()

+    array([[1., 0., 1.],

+           [0., 1., 1.],

+           [1., 0., 1.]])

+

+    See also

+    --------

+    NearestNeighbors.radius_neighbors_graph

+    """

+    if n_neighbors is None:

+        n_neighbors = self.n_neighbors

+

+    # Validate that n_neighbors is an integer

+    if not isinstance(n_neighbors, int) or n_neighbors < 1:

+        raise ValueError("n_neighbors must be a positive integer, "

+                         "got {}".format(n_neighbors))

+

+    # kneighbors does the None handling.

+    if X is not None:

+        X = check_array(X, accept_sparse='csr')

+        n_samples1 = X.shape[0]

+    else:

+        n_samples1 = self._fit_X.shape[0]

+

+    n_samples2 = self._fit_X.shape[0]

+    n_nonzero = n_samples1 * n_neighbors

+    A_indptr = np.arange(0, n_nonzero + 1, n_neighbors)

+

+    # construct CSR matrix representation of the k-NN graph

+    if mode == 'connectivity':

+        A_data = np.ones(n_samples1 * n_neighbors)

+        A_ind = self.kneighbors(X, n_neighbors, return_distance=False)

+

+    elif mode == 'distance':

+        A_data, A_ind = self.kneighbors(

+            X, n_neighbors, return_distance=True)

+        A_data = np.ravel(A_data)

+

+    else:

+        raise ValueError(

+            'Unsupported mode, must be one of "connectivity" '

+            'or "distance" but got "%s" instead' % mode)

+

+    kneighbors_graph = csr_matrix((A_data, A_ind.ravel(), A_indptr),

+                                  shape=(n_samples1, n_samples2))

+

+    return kneighbors_graph

 

 

 class RadiusNeighborsMixin(object):


--- a/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/187/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -565,122 +565,39 @@
     return array

 

 

-def check_X_y(X, y, accept_sparse=False, dtype="numeric", order=None,

-              copy=False, force_all_finite=True, ensure_2d=True,

-              allow_nd=False, multi_output=False, ensure_min_samples=1,

-              ensure_min_features=1, y_numeric=False,

-              warn_on_dtype=False, estimator=None):

-    """Input validation for standard estimators.

-

-    Checks X and y for consistent length, enforces X 2d and y 1d.

-    Standard input checks are only applied to y, such as checking that y

-    does not have np.nan or np.inf targets. For multi-label y, set

-    multi_output=True to allow 2d and sparse y.  If the dtype of X is

-    object, attempt converting to float, raising on failure.

-

-    Parameters

-    ----------

-    X : nd-array, list or sparse matrix

-        Input data.

-

-    y : nd-array, list or sparse matrix

-        Labels.

-

-    accept_sparse : string, boolean or list of string (default=False)

-        String[s] representing allowed sparse matrix formats, such as 'csc',

-        'csr', etc. If the input is sparse but not in the allowed format,

-        it will be converted to the first listed format. True allows the input

-        to be any format. False means that a sparse matrix input will

-        raise an error.

-

-        .. deprecated:: 0.19

-           Passing 'None' to parameter ``accept_sparse`` in methods is

-           deprecated in version 0.19 "and will be removed in 0.21. Use

-           ``accept_sparse=False`` instead.

-

-    dtype : string, type, list of types or None (default="numeric")

-        Data type of result. If None, the dtype of the input is preserved.

-        If "numeric", dtype is preserved unless array.dtype is object.

-        If dtype is a list of types, conversion on the first type is only

-        performed if the dtype of the input is not in the list.

-

-    order : 'F', 'C' or None (default=None)

-        Whether an array will be forced to be fortran or c-style.

-

-    copy : boolean (default=False)

-        Whether a forced copy will be triggered. If copy=False, a copy might

-        be triggered by a conversion.

-

-    force_all_finite : boolean or 'allow-nan', (default=True)

-        Whether to raise an error on np.inf and np.nan in X. This parameter

-        does not influence whether y can have np.inf or np.nan values.

-        The possibilities are:

-

-        - True: Force all values of X to be finite.

-        - False: accept both np.inf and np.nan in X.

-        - 'allow-nan':  accept  only  np.nan  values in  X.  Values  cannot  be

-          infinite.

-

-        .. versionadded:: 0.20

-           ``force_all_finite`` accepts the string ``'allow-nan'``.

-

-    ensure_2d : boolean (default=True)

-        Whether to make X at least 2d.

-

-    allow_nd : boolean (default=False)

-        Whether to allow X.ndim > 2.

-

-    multi_output : boolean (default=False)

-        Whether to allow 2-d y (array or sparse matrix). If false, y will be

-        validated as a vector. y cannot have np.nan or np.inf values if

-        multi_output=True.

-

-    ensure_min_samples : int (default=1)

-        Make sure that X has a minimum number of samples in its first

-        axis (rows for a 2D array).

-

-    ensure_min_features : int (default=1)

-        Make sure that the 2D array has some minimum number of features

-        (columns). The default value of 1 rejects empty datasets.

-        This check is only enforced when X has effectively 2 dimensions or

-        is originally 1D and ``ensure_2d`` is True. Setting to 0 disables

-        this check.

-

-    y_numeric : boolean (default=False)

-        Whether to ensure that y has a numeric type. If dtype of y is object,

-        it is converted to float64. Should only be used for regression

-        algorithms.

-

-    warn_on_dtype : boolean (default=False)

-        Raise DataConversionWarning if the dtype of the input data structure

-        does not match the requested dtype, causing a memory copy.

-

-    estimator : str or estimator instance (default=None)

-        If passed, include the name of the estimator in warning messages.

-

-    Returns

-    -------

-    X_converted : object

-        The converted and validated X.

-

-    y_converted : object

-        The converted and validated y.

-    """

-    X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,

-                    ensure_2d, allow_nd, ensure_min_samples,

-                    ensure_min_features, warn_on_dtype, estimator)

-    if multi_output:

-        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

-                        dtype=None)

-    else:

-        y = column_or_1d(y, warn=True)

-        _assert_all_finite(y)

-    if y_numeric and y.dtype.kind == 'O':

-        y = y.astype(np.float64)

-

-    check_consistent_length(X, y)

-

-    return X, y

+from sklearn.neighbors import NearestNeighbors

+from sklearn.datasets import make_blobs

+from sklearn.utils.validation import check_type

+

+class NearestNeighbors:

+    def __init__(self, n_neighbors, algorithm='auto', leaf_size=30, metric='minkowski',

+                 p=2, metric_params=None, n_jobs=None):

+        self.n_neighbors = n_neighbors

+        self.algorithm = algorithm

+        self.leaf_size = leaf_size

+        self.metric = metric

+        self.p = p

+        self.metric_params = metric_params

+        self.n_jobs = n_jobs

+

+    def fit(self, X):

+        self.n_samples_ = X.shape[0]

+        return self

+

+    def kneighbors(self, X, n_neighbors=None, return_distance=True):

+        if n_neighbors is None:

+            n_neighbors = self.n_neighbors

+        else:

+            check_type(n_neighbors, "n_neighbors", [int])

+        # The rest of the kneighbors method implementation

+        # ...

+        return distances, indices

+

+# Test the fixed function

+X, y = make_blobs()

+neighbors = NearestNeighbors(n_neighbors=3.)

+neighbors.fit(X)

+distances, indices = neighbors.kneighbors(X, n_neighbors=3.)

 

 

 def column_or_1d(y, warn=False):


