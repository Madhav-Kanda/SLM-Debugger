--- a/decoupled/60/0.8_0.2/156/pydata_xarray/xarray/core/dataset.py
+++ b/decoupled/60/0.8_0.2/156/pydata_xarray/xarray/core/dataset.py
@@ -433,105 +433,99 @@
     _coarsen_cls = rolling.DatasetCoarsen

     _resample_cls = resample.DatasetResample

 

-    def __init__(

-        self,

-        # could make a VariableArgs to use more generally, and refine these

-        # categories

-        data_vars: Mapping[Hashable, Any] = None,

-        coords: Mapping[Hashable, Any] = None,

-        attrs: Mapping[Hashable, Any] = None,

-        compat=None,

-    ):

-        """To load data from a file or file-like object, use the `open_dataset`

-        function.

-

-        Parameters

-        ----------

-        data_vars : dict-like, optional

-            A mapping from variable names to :py:class:`~xarray.DataArray`

-            objects, :py:class:`~xarray.Variable` objects or to tuples of the

-            form ``(dims, data[, attrs])`` which can be used as arguments to

-            create a new ``Variable``. Each dimension must have the same length

-            in all variables in which it appears.

-

-            The following notations are accepted:

-

-            - mapping {var name: DataArray}

-            - mapping {var name: Variable}

-            - mapping {var name: (dimension name, array-like)}

-            - mapping {var name: (tuple of dimension names, array-like)}

-            - mapping {dimension name: array-like}

-              (it will be automatically moved to coords, see below)

-

-            Each dimension must have the same length in all variables in which

-            it appears.

-        coords : dict-like, optional

-            Another mapping in similar form as the `data_vars` argument,

-            except the each item is saved on the dataset as a "coordinate".

-            These variables have an associated meaning: they describe

-            constant/fixed/independent quantities, unlike the

-            varying/measured/dependent quantities that belong in `variables`.

-            Coordinates values may be given by 1-dimensional arrays or scalars,

-            in which case `dims` do not need to be supplied: 1D arrays will be

-            assumed to give index values along the dimension with the same

-            name.

-

-            The following notations are accepted:

-

-            - mapping {coord name: DataArray}

-            - mapping {coord name: Variable}

-            - mapping {coord name: (dimension name, array-like)}

-            - mapping {coord name: (tuple of dimension names, array-like)}

-            - mapping {dimension name: array-like}

-              (the dimension name is implicitly set to be the same as the coord name)

-

-            The last notation implies that the coord name is the same as the

-            dimension name.

-

-        attrs : dict-like, optional

-            Global attributes to save on this dataset.

-        compat : deprecated

-        """

-        if compat is not None:

-            warnings.warn(

-                "The `compat` argument to Dataset is deprecated and will be "

-                "removed in 0.15."

-                "Instead, use `merge` to control how variables are combined",

-                FutureWarning,

-                stacklevel=2,

-            )

-        else:

-            compat = "broadcast_equals"

-

-        # TODO(shoyer): expose indexes as a public argument in __init__

-

-        if data_vars is None:

-            data_vars = {}

-        if coords is None:

-            coords = {}

-

-        both_data_and_coords = set(data_vars) & set(coords)

-        if both_data_and_coords:

-            raise ValueError(

-                "variables %r are found in both data_vars and coords"

-                % both_data_and_coords

-            )

-

-        if isinstance(coords, Dataset):

-            coords = coords.variables

-

-        variables, coord_names, dims, indexes = merge_data_and_coords(

-            data_vars, coords, compat=compat

+def __init__(

+    self,

+    # could make a VariableArgs to use more generally, and refine these

+    # categories

+    data_vars: Mapping[Hashable, Any] = None,

+    coords: Mapping[Hashable, Any] = None,

+    attrs: Mapping[Hashable, Any] = None,

+    compat: str = "outer",

+):

+    """To load data from a file or file-like object, use the `open_dataset`

+    function.

+

+    Parameters

+    ----------

+    data_vars : dict-like, optional

+        A mapping from variable names to :py:class:`~xarray.DataArray`

+        objects, :py:class:`~xarray.Variable` objects or to tuples of the

+        form ``(dims, data[, attrs])`` which can be used as arguments to

+        create a new ``Variable``. Each dimension must have the same length

+        in all variables in which it appears.

+

+        The following notations are accepted:

+

+        - mapping {var name: DataArray}

+        - mapping {var name: Variable}

+        - mapping {var name: (dimension name, array-like)}

+        - mapping {var name: (tuple of dimension names, array-like)}

+        - mapping {dimension name: array-like}

+          (it will be automatically moved to coords, see below)

+

+        Each dimension must have the same length in all variables in which

+        it appears.

+    coords : dict-like, optional

+        Another mapping in similar form as the `data_vars` argument,

+        except the each item is saved on the dataset as a "coordinate".

+        These variables have an associated meaning: they describe

+        constant/fixed/independent quantities, unlike the

+        varying/measured/dependent quantities that belong in `variables`.

+        Coordinates values may be given by 1-dimensional arrays or scalars,

+        in which case `dims` do not need to be supplied: 1D arrays will be

+        assumed to give index values along the dimension with the same

+        name.

+

+        The following notations are accepted:

+

+        - mapping {coord name: DataArray}

+        - mapping {coord name: Variable}

+        - mapping {coord name: (dimension name, array-like)}

+        - mapping {coord name: (tuple of dimension names, array-like)}

+        - mapping {dimension name: array-like}

+          (the dimension name is implicitly set to be the same as the coord name)

+

+        The last notation implies that the coord name is the same as the

+        dimension name.

+

+    attrs : dict-like, optional

+        Global attributes to save on this dataset.

+    compat : {'broadcast_equals', 'outer'}, optional

+        Method to use for checking variable compatibility. 'outer' will

+        allow for an outer join-like operation, while 'broadcast_equals'

+        will require variables to be broadcastable to the same shape.

+        Default is 'outer'.

+    """

+    if compat not in {"broadcast_equals", "outer"}:

+        raise ValueError("compat must be one of 'broadcast_equals' or 'outer'")

+

+    if data_vars is None:

+        data_vars = {}

+    if coords is None:

+        coords = {}

+

+    both_data_and_coords = set(data_vars) & set(coords)

+    if both_data_and_coords:

+        raise ValueError(

+            "variables %r are found in both data_vars and coords"

+            % both_data_and_coords

         )

 

-        self._accessors = None

-        self._attrs = dict(attrs) if attrs is not None else None

-        self._file_obj = None

-        self._encoding = None

-        self._variables = variables

-        self._coord_names = coord_names

-        self._dims = dims

-        self._indexes = indexes

+    if isinstance(coords, Dataset):

+        coords = coords.variables

+

+    variables, coord_names, dims, indexes = merge_data_and_coords(

+        data_vars, coords, compat=compat

+    )

+

+    self._accessors = None

+    self._attrs = dict(attrs) if attrs is not None else None

+    self._file_obj = None

+    self._encoding = None

+    self._variables = variables

+    self._coord_names = coord_names

+    self._dims = dims

+    self._indexes = indexes

 

     @classmethod

     def load_store(cls, store, decoder=None) -> "Dataset":
