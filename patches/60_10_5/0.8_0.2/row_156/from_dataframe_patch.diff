--- a/decoupled/60/0.8_0.2/156/pydata_xarray/xarray/core/dataset.py
+++ b/decoupled/60/0.8_0.2/156/pydata_xarray/xarray/core/dataset.py
@@ -4338,63 +4338,101 @@
             self[name] = (dims, data)

 

     @classmethod

-    def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":

-        """Convert a pandas.DataFrame into an xarray.Dataset

-

-        Each column will be converted into an independent variable in the

-        Dataset. If the dataframe's index is a MultiIndex, it will be expanded

-        into a tensor product of one-dimensional indices (filling in missing

-        values with NaN). This method will produce a Dataset very similar to

-        that on which the 'to_dataframe' method was called, except with

-        possibly redundant dimensions (since all dataset variables will have

-        the same dimensionality)

-

-        Parameters

-        ----------

-        dataframe : pandas.DataFrame

-            DataFrame from which to copy data and indices.

-        sparse : bool

-            If true, create a sparse arrays instead of dense numpy arrays. This

-            can potentially save a large amount of memory if the DataFrame has

-            a MultiIndex. Requires the sparse package (sparse.pydata.org).

-

-        Returns

-        -------

-        New Dataset.

-

-        See also

-        --------

-        xarray.DataArray.from_series

-        """

-        # TODO: Add an option to remove dimensions along which the variables

-        # are constant, to enable consistent serialization to/from a dataframe,

-        # even if some variables have different dimensionality.

-

-        if not dataframe.columns.is_unique:

-            raise ValueError("cannot convert DataFrame with non-unique columns")

-

-        idx = dataframe.index

-        obj = cls()

-

-        if isinstance(idx, pd.MultiIndex):

-            dims = tuple(

-                name if name is not None else "level_%i" % n

-                for n, name in enumerate(idx.names)

-            )

-            for dim, lev in zip(dims, idx.levels):

-                obj[dim] = (dim, lev)

-            shape = tuple(lev.size for lev in idx.levels)

-        else:

-            index_name = idx.name if idx.name is not None else "index"

-            dims = (index_name,)

-            obj[index_name] = (dims, idx)

-            shape = (idx.size,)

-

-        if sparse:

-            obj._set_sparse_data_from_dataframe(dataframe, dims, shape)

-        else:

-            obj._set_numpy_data_from_dataframe(dataframe, dims, shape)

-        return obj

+def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":

+    """Convert a pandas.DataFrame into an xarray.Dataset

+

+    Each column will be converted into an independent variable in the

+    Dataset. If the dataframe's index is a MultiIndex, it will be expanded

+    into a tensor product of one-dimensional indices (filling in missing

+    values with NaN). This method will produce a Dataset very similar to

+    that on which the 'to_dataframe' method was called, except with

+    possibly redundant dimensions (since all dataset variables will have

+    the same dimensionality)

+

+    Parameters

+    ----------

+    dataframe : pandas.DataFrame

+        DataFrame from which to copy data and indices.

+    sparse : bool

+        If true, create a sparse arrays instead of dense numpy arrays. This

+        can potentially save a large amount of memory if the DataFrame has

+        a MultiIndex. Requires the sparse package (sparse.pydata.org).

+

+    Returns

+    -------

+    New Dataset.

+

+    See also

+    --------

+    xarray.DataArray.from_series

+    """

+    # TODO: Add an option to remove dimensions along which the variables

+    # are constant, to enable consistent serialization to/from a dataframe,

+    # even if some variables have different dimensionality.

+

+    if not dataframe.columns.is_unique:

+        raise ValueError("cannot convert DataFrame with non-unique columns")

+

+    idx = dataframe.index

+    obj = cls()

+

+    if isinstance(idx, pd.MultiIndex):

+        dims = tuple(

+            name if name is not None else "level_%i" % n

+            for n, name in enumerate(idx.names)

+        )

+        for dim, lev in zip(dims, idx.levels):

+            obj[dim] = (dim, lev)

+        shape = tuple(lev.size for lev in idx.levels)

+    else:

+        index_name = idx.name if idx.name is not None else "index"

+        dims = (index_name,)

+        obj[index_name] = (dims, idx)

+        shape = (idx.size,)

+

+    if sparse:

+        obj._set_sparse_data_from_dataframe(dataframe, dims, shape)

+    else:

+        obj._set_numpy_data_from_dataframe(dataframe, dims, shape)

+    return obj

+

+def concatenate_datasets(datasets: List["Dataset"], fill_value: Optional[float] = None) -> "Dataset":

+    """

+    Concatenate multiple xarray Datasets along the first dimension.

+

+    Parameters

+    ----------

+    datasets : List["Dataset"]

+        List of xarray Datasets to concatenate.

+    fill_value : Optional[float]

+        Value to fill in for missing variables in the concatenated dataset.

+

+    Returns

+    -------

+    New Dataset.

+    """

+    if not datasets:

+        raise ValueError("At least one dataset must be provided")

+

+    # Determine the common dimensions and variables

+    common_dims = set.intersection(*[set(ds.dims) for ds in datasets])

+    common_vars = set.intersection(*[set(ds.variables) for ds in datasets])

+

+    # Create a new Dataset with the common dimensions and variables

+    new_dataset = Dataset()

+

+    # Add common dimensions

+    for dim in common_dims:

+        new_dataset[dim] = (dim, np.concatenate([ds[dim].values for ds in datasets]))

+

+    # Add common variables

+    for var in common_vars:

+        data = np.concatenate([ds[var].values for ds in datasets], axis=0)

+        if fill_value is not None:

+            data = np.where(np.isnan(data), fill_value, data)

+        new_dataset[var] = (ds[var].dims, data)

+

+    return new_dataset

 

     def to_dask_dataframe(self, dim_order=None, set_index=False):

         """
