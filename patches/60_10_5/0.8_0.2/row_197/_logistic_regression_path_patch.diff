--- a/decoupled/60/0.8_0.2/197/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
+++ b/decoupled/60/0.8_0.2/197/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
@@ -650,7 +650,7 @@
                               intercept_scaling=1., multi_class='auto',

                               random_state=None, check_input=True,

                               max_squared_sum=None, sample_weight=None,

-                              l1_ratio=None):

+                              l1_ratio=None, refit=False):

     """Compute a Logistic Regression model for a list of regularization

     parameters.

 

@@ -722,273 +722,7 @@
     penalty : str, 'l1', 'l2', or 'elasticnet'

         Used to specify the norm used in the penalization. The 'newton-cg',

         'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is

-        only supported by the 'saga' solver.

-

-    intercept_scaling : float, default 1.

-        Useful only when the solver 'liblinear' is used

-        and self.fit_intercept is set to True. In this case, x becomes

-        [x, self.intercept_scaling],

-        i.e. a "synthetic" feature with constant value equal to

-        intercept_scaling is appended to the instance vector.

-        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

-

-        Note! the synthetic feature weight is subject to l1/l2 regularization

-        as all other features.

-        To lessen the effect of regularization on synthetic feature weight

-        (and therefore on the intercept) intercept_scaling has to be increased.

-

-    multi_class : {'ovr', 'multinomial', 'auto'}, default='auto'

-        If the option chosen is 'ovr', then a binary problem is fit for each

-        label. For 'multinomial' the loss minimised is the multinomial loss fit

-        across the entire probability distribution, *even when the data is

-        binary*. 'multinomial' is unavailable when solver='liblinear'.

-        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',

-        and otherwise selects 'multinomial'.

-

-        .. versionadded:: 0.18

-           Stochastic Average Gradient descent solver for 'multinomial' case.

-        .. versionchanged:: 0.22

-            Default changed from 'ovr' to 'auto' in 0.22.

-

-    random_state : int, RandomState instance or None, optional, default None

-        The seed of the pseudo random number generator to use when shuffling

-        the data.  If int, random_state is the seed used by the random number

-        generator; If RandomState instance, random_state is the random number

-        generator; If None, the random number generator is the RandomState

-        instance used by `np.random`. Used when ``solver`` == 'sag' or

-        'liblinear'.

-

-    check_input : bool, default True

-        If False, the input arrays X and y will not be checked.

-

-    max_squared_sum : float, default None

-        Maximum squared sum of X over samples. Used only in SAG solver.

-        If None, it will be computed, going through all the samples.

-        The value should be precomputed to speed up cross validation.

-

-    sample_weight : array-like, shape(n_samples,) optional

-        Array of weights that are assigned to individual samples.

-        If not provided, then each sample is given unit weight.

-

-    l1_ratio : float or None, optional (default=None)

-        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only

-        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent

-        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent

-        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a

-        combination of L1 and L2.

-

-    Returns

-    -------

-    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)

-        List of coefficients for the Logistic Regression model. If

-        fit_intercept is set to True then the second dimension will be

-        n_features + 1, where the last item represents the intercept. For

-        ``multiclass='multinomial'``, the shape is (n_classes, n_cs,

-        n_features) or (n_classes, n_cs, n_features + 1).

-

-    Cs : ndarray

-        Grid of Cs used for cross-validation.

-

-    n_iter : array, shape (n_cs,)

-        Actual number of iteration for each Cs.

-

-    Notes

-    -----

-    You might get slightly different results with the solver liblinear than

-    with the others since this uses LIBLINEAR which penalizes the intercept.

-

-    .. versionchanged:: 0.19

-        The "copy" parameter was removed.

-    """

-    if isinstance(Cs, numbers.Integral):

-        Cs = np.logspace(-4, 4, Cs)

-

-    solver = _check_solver(solver, penalty, dual)

-

-    # Preprocessing.

-    if check_input:

-        X = check_array(X, accept_sparse='csr', dtype=np.float64,

-                        accept_large_sparse=solver != 'liblinear')

-        y = check_array(y, ensure_2d=False, dtype=None)

-        check_consistent_length(X, y)

-    _, n_features = X.shape

-

-    classes = np.unique(y)

-    random_state = check_random_state(random_state)

-

-    multi_class = _check_multi_class(multi_class, solver, len(classes))

-    if pos_class is None and multi_class != 'multinomial':

-        if (classes.size > 2):

-            raise ValueError('To fit OvR, use the pos_class argument')

-        # np.unique(y) gives labels in sorted order.

-        pos_class = classes[1]

-

-    # If sample weights exist, convert them to array (support for lists)

-    # and check length

-    # Otherwise set them to 1 for all examples

-    if sample_weight is not None:

-        sample_weight = np.array(sample_weight, dtype=X.dtype, order='C')

-        check_consistent_length(y, sample_weight)

-    else:

-        sample_weight = np.ones(X.shape[0], dtype=X.dtype)

-

-    # If class_weights is a dict (provided by the user), the weights

-    # are assigned to the original labels. If it is "balanced", then

-    # the class_weights are assigned after masking the labels with a OvR.

-    le = LabelEncoder()

-    if isinstance(class_weight, dict) or multi_class == 'multinomial':

-        class_weight_ = compute_class_weight(class_weight, classes, y)

-        sample_weight *= class_weight_[le.fit_transform(y)]

-

-    # For doing a ovr, we need to mask the labels first. for the

-    # multinomial case this is not necessary.

-    if multi_class == 'ovr':

-        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)

-        mask_classes = np.array([-1, 1])

-        mask = (y == pos_class)

-        y_bin = np.ones(y.shape, dtype=X.dtype)

-        y_bin[~mask] = -1.

-        # for compute_class_weight

-

-        if class_weight == "balanced":

-            class_weight_ = compute_class_weight(class_weight, mask_classes,

-                                                 y_bin)

-            sample_weight *= class_weight_[le.fit_transform(y_bin)]

-

-    else:

-        if solver not in ['sag', 'saga']:

-            lbin = LabelBinarizer()

-            Y_multi = lbin.fit_transform(y)

-            if Y_multi.shape[1] == 1:

-                Y_multi = np.hstack([1 - Y_multi, Y_multi])

-        else:

-            # SAG multinomial solver needs LabelEncoder, not LabelBinarizer

-            le = LabelEncoder()

-            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)

-

-        w0 = np.zeros((classes.size, n_features + int(fit_intercept)),

-                      order='F', dtype=X.dtype)

-

-    if coef is not None:

-        # it must work both giving the bias term and not

-        if multi_class == 'ovr':

-            if coef.size not in (n_features, w0.size):

-                raise ValueError(

-                    'Initialization coef is of shape %d, expected shape '

-                    '%d or %d' % (coef.size, n_features, w0.size))

-            w0[:coef.size] = coef

-        else:

-            # For binary problems coef.shape[0] should be 1, otherwise it

-            # should be classes.size.

-            n_classes = classes.size

-            if n_classes == 2:

-                n_classes = 1

-

-            if (coef.shape[0] != n_classes or

-                    coef.shape[1] not in (n_features, n_features + 1)):

-                raise ValueError(

-                    'Initialization coef is of shape (%d, %d), expected '

-                    'shape (%d, %d) or (%d, %d)' % (

-                        coef.shape[0], coef.shape[1], classes.size,

-                        n_features, classes.size, n_features + 1))

-

-            if n_classes == 1:

-                w0[0, :coef.shape[1]] = -coef

-                w0[1, :coef.shape[1]] = coef

-            else:

-                w0[:, :coef.shape[1]] = coef

-

-    if multi_class == 'multinomial':

-        # fmin_l_bfgs_b and newton-cg accepts only ravelled parameters.

-        if solver in ['lbfgs', 'newton-cg']:

-            w0 = w0.ravel()

-        target = Y_multi

-        if solver == 'lbfgs':

-            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]

-        elif solver == 'newton-cg':

-            func = lambda x, *args: _multinomial_loss(x, *args)[0]

-            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]

-            hess = _multinomial_grad_hess

-        warm_start_sag = {'coef': w0.T}

-    else:

-        target = y_bin

-        if solver == 'lbfgs':

-            func = _logistic_loss_and_grad

-        elif solver == 'newton-cg':

-            func = _logistic_loss

-            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]

-            hess = _logistic_grad_hess

-        warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}

-

-    coefs = list()

-    n_iter = np.zeros(len(Cs), dtype=np.int32)

-    for i, C in enumerate(Cs):

-        if solver == 'lbfgs':

-            iprint = [-1, 50, 1, 100, 101][

-                np.searchsorted(np.array([0, 1, 2, 3]), verbose)]

-            w0, loss, info = optimize.fmin_l_bfgs_b(

-                func, w0, fprime=None,

-                args=(X, target, 1. / C, sample_weight),

-                iprint=iprint, pgtol=tol, maxiter=max_iter)

-            if info["warnflag"] == 1:

-                warnings.warn("lbfgs failed to converge. Increase the number "

-                              "of iterations.", ConvergenceWarning)

-            # In scipy <= 1.0.0, nit may exceed maxiter.

-            # See https://github.com/scipy/scipy/issues/7854.

-            n_iter_i = min(info['nit'], max_iter)

-        elif solver == 'newton-cg':

-            args = (X, target, 1. / C, sample_weight)

-            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,

-                                     maxiter=max_iter, tol=tol)

-        elif solver == 'liblinear':

-            coef_, intercept_, n_iter_i, = _fit_liblinear(

-                X, target, C, fit_intercept, intercept_scaling, None,

-                penalty, dual, verbose, max_iter, tol, random_state,

-                sample_weight=sample_weight)

-            if fit_intercept:

-                w0 = np.concatenate([coef_.ravel(), intercept_])

-            else:

-                w0 = coef_.ravel()

-

-        elif solver in ['sag', 'saga']:

-            if multi_class == 'multinomial':

-                target = target.astype(X.dtype, copy=False)

-                loss = 'multinomial'

-            else:

-                loss = 'log'

-            # alpha is for L2-norm, beta is for L1-norm

-            if penalty == 'l1':

-                alpha = 0.

-                beta = 1. / C

-            elif penalty == 'l2':

-                alpha = 1. / C

-                beta = 0.

-            else:  # Elastic-Net penalty

-                alpha = (1. / C) * (1 - l1_ratio)

-                beta = (1. / C) * l1_ratio

-

-            w0, n_iter_i, warm_start_sag = sag_solver(

-                X, target, sample_weight, loss, alpha,

-                beta, max_iter, tol,

-                verbose, random_state, False, max_squared_sum, warm_start_sag,

-                is_saga=(solver == 'saga'))

-

-        else:

-            raise ValueError("solver must be one of {'liblinear', 'lbfgs', "

-                             "'newton-cg', 'sag'}, got '%s' instead" % solver)

-

-        if multi_class == 'multinomial':

-            n_classes = max(2, classes.size)

-            multi_w0 = np.reshape(w0, (n_classes, -1))

-            if n_classes == 2:

-                multi_w0 = multi_w0[1][np.newaxis, :]

-            coefs.append(multi_w0.copy())

-        else:

-            coefs.append(w0.copy())

-

-        n_iter[i] = n_iter_i

-

-    return np.array(coefs), np.array(Cs), n_iter

+        only supported by the 'saga' solver

 

 

 # helper function for LogisticCV
