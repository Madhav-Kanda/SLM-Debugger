--- a/decoupled/60/0.8_0.2/197/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
+++ b/decoupled/60/0.8_0.2/197/scikit-learn_scikit-learn/sklearn/linear_model/logistic.py
@@ -1933,283 +1933,7 @@
         self.random_state = random_state

         self.l1_ratios = l1_ratios

 

-    def fit(self, X, y, sample_weight=None):

-        """Fit the model according to the given training data.

-

-        Parameters

-        ----------

-        X : {array-like, sparse matrix}, shape (n_samples, n_features)

-            Training vector, where n_samples is the number of samples and

-            n_features is the number of features.

-

-        y : array-like, shape (n_samples,)

-            Target vector relative to X.

-

-        sample_weight : array-like, shape (n_samples,) optional

-            Array of weights that are assigned to individual samples.

-            If not provided, then each sample is given unit weight.

-

-        Returns

-        -------

-        self : object

-        """

-        solver = _check_solver(self.solver, self.penalty, self.dual)

-

-        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:

-            raise ValueError("Maximum number of iteration must be positive;"

-                             " got (max_iter=%r)" % self.max_iter)

-        if not isinstance(self.tol, numbers.Number) or self.tol < 0:

-            raise ValueError("Tolerance for stopping criteria must be "

-                             "positive; got (tol=%r)" % self.tol)

-        if self.penalty == 'elasticnet':

-            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(

-                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0

-                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):

-                raise ValueError("l1_ratios must be a list of numbers between "

-                                 "0 and 1; got (l1_ratios=%r)" %

-                                 self.l1_ratios)

-            l1_ratios_ = self.l1_ratios

-        else:

-            if self.l1_ratios is not None:

-                warnings.warn("l1_ratios parameter is only used when penalty "

-                              "is 'elasticnet'. Got (penalty={})".format(

-                                  self.penalty))

-

-            l1_ratios_ = [None]

-

-        if self.penalty == 'none':

-            raise ValueError(

-                "penalty='none' is not useful and not supported by "

-                "LogisticRegressionCV."

-            )

-

-        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,

-                         order="C",

-                         accept_large_sparse=solver != 'liblinear')

-        check_classification_targets(y)

-

-        class_weight = self.class_weight

-

-        # Encode for string labels

-        label_encoder = LabelEncoder().fit(y)

-        y = label_encoder.transform(y)

-        if isinstance(class_weight, dict):

-            class_weight = {label_encoder.transform([cls])[0]: v

-                            for cls, v in class_weight.items()}

-

-        # The original class labels

-        classes = self.classes_ = label_encoder.classes_

-        encoded_labels = label_encoder.transform(label_encoder.classes_)

-

-        multi_class = _check_multi_class(self.multi_class, solver,

-                                         len(classes))

-

-        if solver in ['sag', 'saga']:

-            max_squared_sum = row_norms(X, squared=True).max()

-        else:

-            max_squared_sum = None

-

-        # init cross-validation generator

-        cv = check_cv(self.cv, y, classifier=True)

-        folds = list(cv.split(X, y))

-

-        # Use the label encoded classes

-        n_classes = len(encoded_labels)

-

-        if n_classes < 2:

-            raise ValueError("This solver needs samples of at least 2 classes"

-                             " in the data, but the data contains only one"

-                             " class: %r" % classes[0])

-

-        if n_classes == 2:

-            # OvR in case of binary problems is as good as fitting

-            # the higher label

-            n_classes = 1

-            encoded_labels = encoded_labels[1:]

-            classes = classes[1:]

-

-        # We need this hack to iterate only once over labels, in the case of

-        # multi_class = multinomial, without changing the value of the labels.

-        if multi_class == 'multinomial':

-            iter_encoded_labels = iter_classes = [None]

-        else:

-            iter_encoded_labels = encoded_labels

-            iter_classes = classes

-

-        # compute the class weights for the entire dataset y

-        if class_weight == "balanced":

-            class_weight = compute_class_weight(class_weight,

-                                                np.arange(len(self.classes_)),

-                                                y)

-            class_weight = dict(enumerate(class_weight))

-

-        path_func = delayed(_log_reg_scoring_path)

-

-        # The SAG solver releases the GIL so it's more efficient to use

-        # threads for this solver.

-        if self.solver in ['sag', 'saga']:

-            prefer = 'threads'

-        else:

-            prefer = 'processes'

-

-        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,

-                               **_joblib_parallel_args(prefer=prefer))(

-            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,

-                      fit_intercept=self.fit_intercept, penalty=self.penalty,

-                      dual=self.dual, solver=solver, tol=self.tol,

-                      max_iter=self.max_iter, verbose=self.verbose,

-                      class_weight=class_weight, scoring=self.scoring,

-                      multi_class=multi_class,

-                      intercept_scaling=self.intercept_scaling,

-                      random_state=self.random_state,

-                      max_squared_sum=max_squared_sum,

-                      sample_weight=sample_weight,

-                      l1_ratio=l1_ratio

-                      )

-            for label in iter_encoded_labels

-            for train, test in folds

-            for l1_ratio in l1_ratios_)

-

-        # _log_reg_scoring_path will output different shapes depending on the

-        # multi_class param, so we need to reshape the outputs accordingly.

-        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the

-        # rows are equal, so we just take the first one.

-        # After reshaping,

-        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)

-        # - coefs_paths is of shape

-        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)

-        # - n_iter is of shape

-        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or

-        #  (1, n_folds, n_Cs . n_l1_ratios)

-        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)

-        self.Cs_ = Cs[0]

-        if multi_class == 'multinomial':

-            coefs_paths = np.reshape(

-                coefs_paths,

-                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)

-            )

-            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),

-            #                                                 (1, 2, 0, 3))

-            coefs_paths = np.swapaxes(coefs_paths, 0, 1)

-            coefs_paths = np.swapaxes(coefs_paths, 0, 2)

-            self.n_iter_ = np.reshape(

-                n_iter_,

-                (1, len(folds), len(self.Cs_) * len(l1_ratios_))

-            )

-            # repeat same scores across all classes

-            scores = np.tile(scores, (n_classes, 1, 1))

-        else:

-            coefs_paths = np.reshape(

-                coefs_paths,

-                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),

-                 -1)

-            )

-            self.n_iter_ = np.reshape(

-                n_iter_,

-                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))

-            )

-        scores = np.reshape(scores, (n_classes, len(folds), -1))

-        self.scores_ = dict(zip(classes, scores))

-        self.coefs_paths_ = dict(zip(classes, coefs_paths))

-

-        self.C_ = list()

-        self.l1_ratio_ = list()

-        self.coef_ = np.empty((n_classes, X.shape[1]))

-        self.intercept_ = np.zeros(n_classes)

-        for index, (cls, encoded_label) in enumerate(

-                zip(iter_classes, iter_encoded_labels)):

-

-            if multi_class == 'ovr':

-                scores = self.scores_[cls]

-                coefs_paths = self.coefs_paths_[cls]

-            else:

-                # For multinomial, all scores are the same across classes

-                scores = scores[0]

-                # coefs_paths will keep its original shape because

-                # logistic_regression_path expects it this way

-

-            if self.refit:

-                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)

-                # for example, with n_cs=2 and n_l1_ratios=3

-                # the layout of scores is

-                # [c1, c2, c1, c2, c1, c2]

-                #   l1_1 ,  l1_2 ,  l1_3

-                best_index = scores.sum(axis=0).argmax()

-

-                best_index_C = best_index % len(self.Cs_)

-                C_ = self.Cs_[best_index_C]

-                self.C_.append(C_)

-

-                best_index_l1 = best_index // len(self.Cs_)

-                l1_ratio_ = l1_ratios_[best_index_l1]

-                self.l1_ratio_.append(l1_ratio_)

-

-                if multi_class == 'multinomial':

-                    coef_init = np.mean(coefs_paths[:, :, best_index, :],

-                                        axis=1)

-                else:

-                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)

-

-                # Note that y is label encoded and hence pos_class must be

-                # the encoded label / None (for 'multinomial')

-                w, _, _ = _logistic_regression_path(

-                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,

-                    fit_intercept=self.fit_intercept, coef=coef_init,

-                    max_iter=self.max_iter, tol=self.tol,

-                    penalty=self.penalty,

-                    class_weight=class_weight,

-                    multi_class=multi_class,

-                    verbose=max(0, self.verbose - 1),

-                    random_state=self.random_state,

-                    check_input=False, max_squared_sum=max_squared_sum,

-                    sample_weight=sample_weight,

-                    l1_ratio=l1_ratio_)

-                w = w[0]

-

-            else:

-                # Take the best scores across every fold and the average of

-                # all coefficients corresponding to the best scores.

-                best_indices = np.argmax(scores, axis=1)

-                if self.multi_class == 'ovr':

-                    w = np.mean([coefs_paths[i, best_indices[i], :]

-                                 for i in range(len(folds))], axis=0)

-                else:

-                    w = np.mean([coefs_paths[:, i, best_indices[i], :]

-                                 for i in range(len(folds))], axis=0)

-

-                best_indices_C = best_indices % len(self.Cs_)

-                self.C_.append(np.mean(self.Cs_[best_indices_C]))

-

-                best_indices_l1 = best_indices // len(self.Cs_)

-                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))

-

-            if multi_class == 'multinomial':

-                self.C_ = np.tile(self.C_, n_classes)

-                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)

-                self.coef_ = w[:, :X.shape[1]]

-                if self.fit_intercept:

-                    self.intercept_ = w[:, -1]

-            else:

-                self.coef_[index] = w[: X.shape[1]]

-                if self.fit_intercept:

-                    self.intercept_[index] = w[-1]

-

-        self.C_ = np.asarray(self.C_)

-        self.l1_ratio_ = np.asarray(self.l1_ratio_)

-        self.l1_ratios_ = np.asarray(l1_ratios_)

-        # if elasticnet was used, add the l1_ratios dimension to some

-        # attributes

-        if self.l1_ratios is not None:

-            for cls, coefs_path in self.coefs_paths_.items():

-                self.coefs_paths_[cls] = coefs_path.reshape(

-                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))

-            for cls, score in self.scores_.items():

-                self.scores_[cls] = score.reshape(

-                    (len(folds), self.Cs_.size, self.l1_ratios_.size))

-            self.n_iter_ = self.n_iter_.reshape(

-                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))

-

-        return self

+w = np.mean([coefs_paths[:, i, best_indices[i], :] for i in range(len(folds))], axis=0)

 

     def score(self, X, y, sample_weight=None):

         """Returns the score using the `scoring` option on the given
