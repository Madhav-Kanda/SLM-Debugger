--- a/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -853,6 +853,10 @@
     test : array-like

         This is the value of the test parameter

     """

+    # Check if estimator is None

+    if estimator is None:

+        raise ValueError("Estimator cannot be None")

+

     # Adjust length of sample weights

     fit_params = fit_params if fit_params is not None else {}

     fit_params = {k: _index_param_value(X, v, train)


--- a/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/estimator_checks.py
+++ b/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/estimator_checks.py
@@ -574,6 +574,19 @@
 

 

 @ignore_warnings(category=(DeprecationWarning, FutureWarning))

+from sklearn.utils.estimator_checks import check_estimator

+from sklearn.utils.validation import has_fit_parameter

+from sklearn.utils import check_X_y

+from sklearn.utils import column_or_1d

+from sklearn.base import clone

+import numpy as np

+from sklearn.datasets import load_iris

+from sklearn.linear_model import LogisticRegression

+from sklearn.ensemble import RandomForestClassifier

+from sklearn.utils.estimator_checks import check_estimator

+from sklearn.utils.validation import check_is_fitted

+from sklearn.utils.validation import _check_sample_weight

+

 def check_sample_weights_list(name, estimator_orig):

     # check that estimators will accept a 'sample_weight' parameter of

     # type list in the 'fit' function.

@@ -587,6 +600,23 @@
         sample_weight = [3] * 10

         # Test that estimators don't raise any exception

         estimator.fit(X, y, sample_weight=sample_weight)

+

+def fit_voting_classifier_with_sample_weights(voter, X, y, sample_weight):

+    # Filter out None estimators before fitting

+    non_none_estimators = {name: est for name, est in voter.estimators_ if est is not None}

+    voter.estimators_ = list(non_none_estimators.items())

+    voter.fit(X, y, sample_weight=sample_weight)

+    voter.estimators_ = list(voter.estimators_)

+    return voter

+

+X, y = load_iris(return_X_y=True)

+voter = VotingClassifier(

+    estimators=[('lr', LogisticRegression()),

+                ('rf', RandomForestClassifier())]

+)

+voter.fit(X, y, sample_weight=np.ones(y.shape))

+voter.set_params(lr=None)

+voter = fit_voting_classifier_with_sample_weights(voter, X, y, sample_weight=np.ones(y.shape))

 

 

 @ignore_warnings(category=(DeprecationWarning, FutureWarning))


--- a/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
+++ b/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/model_selection/_validation.py
@@ -507,84 +507,9 @@
 

     is_multimetric = not callable(scorer)

     n_scorers = len(scorer.keys()) if is_multimetric else 1

-    try:

-        if y_train is None:

-            estimator.fit(X_train, **fit_params)

-        else:

-            estimator.fit(X_train, y_train, **fit_params)

-

-    except Exception as e:

-        # Note fit time as time until error

-        fit_time = time.time() - start_time

-        score_time = 0.0

-        if error_score == 'raise':

-            raise

-        elif error_score == 'raise-deprecating':

-            warnings.warn("From version 0.22, errors during fit will result "

-                          "in a cross validation score of NaN by default. Use "

-                          "error_score='raise' if you want an exception "

-                          "raised or error_score=np.nan to adopt the "

-                          "behavior from version 0.22.",

-                          FutureWarning)

-            raise

-        elif isinstance(error_score, numbers.Number):

-            if is_multimetric:

-                test_scores = dict(zip(scorer.keys(),

-                                   [error_score, ] * n_scorers))

-                if return_train_score:

-                    train_scores = dict(zip(scorer.keys(),

-                                        [error_score, ] * n_scorers))

-            else:

-                test_scores = error_score

-                if return_train_score:

-                    train_scores = error_score

-            warnings.warn("Estimator fit failed. The score on this train-test"

-                          " partition for these parameters will be set to %f. "

-                          "Details: \n%s" %

-                          (error_score, format_exception_only(type(e), e)[0]),

-                          FitFailedWarning)

-        else:

-            raise ValueError("error_score must be the string 'raise' or a"

-                             " numeric value. (Hint: if using 'raise', please"

-                             " make sure that it has been spelled correctly.)")

-

-    else:

-        fit_time = time.time() - start_time

-        # _score will return dict if is_multimetric is True

-        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)

-        score_time = time.time() - start_time - fit_time

-        if return_train_score:

-            train_scores = _score(estimator, X_train, y_train, scorer,

-                                  is_multimetric)

-    if verbose > 2:

-        if is_multimetric:

-            for scorer_name in sorted(test_scores):

-                msg += ", %s=" % scorer_name

-                if return_train_score:

-                    msg += "(train=%.3f," % train_scores[scorer_name]

-                    msg += " test=%.3f)" % test_scores[scorer_name]

-                else:

-                    msg += "%.3f" % test_scores[scorer_name]

-        else:

-            msg += ", score="

-            msg += ("%.3f" % test_scores if not return_train_score else

-                    "(train=%.3f, test=%.3f)" % (train_scores, test_scores))

-

-    if verbose > 1:

-        total_time = score_time + fit_time

-        print(_message_with_time('CV', msg, total_time))

-

-    ret = [train_scores, test_scores] if return_train_score else [test_scores]

-

-    if return_n_test_samples:

-        ret.append(_num_samples(X_test))

-    if return_times:

-        ret.extend([fit_time, score_time])

-    if return_parameters:

-        ret.append(parameters)

-    if return_estimator:

-        ret.append(estimator)

-    return ret

+

+    # Check if estimator is None before fitting

+    if estimator is

 

 

 def _score(estimator, X_test, y_test, scorer, is_multimetric=False):


--- a/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -863,55 +863,27 @@
     return array

 

 

-def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):

-    """Perform is_fitted validation for estimator.

-

-    Checks if the estimator is fitted by verifying the presence of

-    "all_or_any" of the passed attributes and raises a NotFittedError with the

-    given message.

-

-    Parameters

-    ----------

-    estimator : estimator instance.

-        estimator instance for which the check is performed.

-

-    attributes : attribute name(s) given as string or a list/tuple of strings

-        Eg.:

-            ``["coef_", "estimator_", ...], "coef_"``

-

-    msg : string

-        The default error message is, "This %(name)s instance is not fitted

-        yet. Call 'fit' with appropriate arguments before using this method."

-

-        For custom messages if "%(name)s" is present in the message string,

-        it is substituted for the estimator name.

-

-        Eg. : "Estimator, %(name)s, must be fitted before sparsifying".

-

-    all_or_any : callable, {all, any}, default all

-        Specify whether all or any of the given attributes must exist.

-

-    Returns

-    -------

-    None

-

-    Raises

-    ------

-    NotFittedError

-        If the attributes are not found.

-    """

-    if msg is None:

-        msg = ("This %(name)s instance is not fitted yet. Call 'fit' with "

-               "appropriate arguments before using this method.")

-

-    if not hasattr(estimator, 'fit'):

-        raise TypeError("%s is not an estimator instance." % (estimator))

-

-    if not isinstance(attributes, (list, tuple)):

-        attributes = [attributes]

-

-    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):

-        raise NotFittedError(msg % {'name': type(estimator).__name__})

+from sklearn.utils.validation import NotFittedError

+from sklearn.utils.validation import check_is_fitted

+

+class VotingClassifier:

+    def __init__(self, estimators):

+        self.estimators = estimators

+        self.estimators_names = [name for name, _ in estimators]

+        self.estimators_ = []

+

+    def fit(self, X, y, sample_weight=None):

+        self.estimators_ = []

+        for name, estimator in self.estimators:

+            if estimator is not None:

+                if sample_weight is not None:

+                    estimator.fit(X, y, sample_weight=sample_weight)

+                else:

+                    estimator.fit(X, y)

+                check_is_fitted(estimator)

+                self.estimators_.append((name, estimator))

+            else:

+                raise ValueError(f"Estimator '{name}' is None and cannot be fitted.")

 

 

 def check_non_negative(X, whom):


--- a/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/196/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -409,173 +409,7 @@
 

     warn_on_dtype : boolean or None, optional (default=None)

         Raise DataConversionWarning if the dtype of the input data structure

-        does not match the requested dtype, causing a memory copy.

-

-        .. deprecated:: 0.21

-            ``warn_on_dtype`` is deprecated in version 0.21 and will be

-            removed in 0.23.

-

-    estimator : str or estimator instance (default=None)

-        If passed, include the name of the estimator in warning messages.

-

-    Returns

-    -------

-    array_converted : object

-        The converted and validated array.

-    """

-    # warn_on_dtype deprecation

-    if warn_on_dtype is not None:

-        warnings.warn(

-            "'warn_on_dtype' is deprecated in version 0.21 and will be "

-            "removed in 0.23. Don't set `warn_on_dtype` to remove this "

-            "warning.",

-            DeprecationWarning)

-

-    # store reference to original array to check if copy is needed when

-    # function returns

-    array_orig = array

-

-    # store whether originally we wanted numeric dtype

-    dtype_numeric = isinstance(dtype, str) and dtype == "numeric"

-

-    dtype_orig = getattr(array, "dtype", None)

-    if not hasattr(dtype_orig, 'kind'):

-        # not a data type (e.g. a column named dtype in a pandas DataFrame)

-        dtype_orig = None

-

-    # check if the object contains several dtypes (typically a pandas

-    # DataFrame), and store them. If not, store None.

-    dtypes_orig = None

-    if hasattr(array, "dtypes") and hasattr(array.dtypes, '__array__'):

-        dtypes_orig = np.array(array.dtypes)

-

-    if dtype_numeric:

-        if dtype_orig is not None and dtype_orig.kind == "O":

-            # if input is object, convert to float.

-            dtype = np.float64

-        else:

-            dtype = None

-

-    if isinstance(dtype, (list, tuple)):

-        if dtype_orig is not None and dtype_orig in dtype:

-            # no dtype conversion required

-            dtype = None

-        else:

-            # dtype conversion required. Let's select the first element of the

-            # list of accepted types.

-            dtype = dtype[0]

-

-    if force_all_finite not in (True, False, 'allow-nan'):

-        raise ValueError('force_all_finite should be a bool or "allow-nan"'

-                         '. Got {!r} instead'.format(force_all_finite))

-

-    if estimator is not None:

-        if isinstance(estimator, str):

-            estimator_name = estimator

-        else:

-            estimator_name = estimator.__class__.__name__

-    else:

-        estimator_name = "Estimator"

-    context = " by %s" % estimator_name if estimator is not None else ""

-

-    if sp.issparse(array):

-        _ensure_no_complex_data(array)

-        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,

-                                      dtype=dtype, copy=copy,

-                                      force_all_finite=force_all_finite,

-                                      accept_large_sparse=accept_large_sparse)

-    else:

-        # If np.array(..) gives ComplexWarning, then we convert the warning

-        # to an error. This is needed because specifying a non complex

-        # dtype to the function converts complex to real dtype,

-        # thereby passing the test made in the lines following the scope

-        # of warnings context manager.

-        with warnings.catch_warnings():

-            try:

-                warnings.simplefilter('error', ComplexWarning)

-                array = np.asarray(array, dtype=dtype, order=order)

-            except ComplexWarning:

-                raise ValueError("Complex data not supported\n"

-                                 "{}\n".format(array))

-

-        # It is possible that the np.array(..) gave no warning. This happens

-        # when no dtype conversion happened, for example dtype = None. The

-        # result is that np.array(..) produces an array of complex dtype

-        # and we need to catch and raise exception for such cases.

-        _ensure_no_complex_data(array)

-

-        if ensure_2d:

-            # If input is scalar raise error

-            if array.ndim == 0:

-                raise ValueError(

-                    "Expected 2D array, got scalar array instead:\narray={}.\n"

-                    "Reshape your data either using array.reshape(-1, 1) if "

-                    "your data has a single feature or array.reshape(1, -1) "

-                    "if it contains a single sample.".format(array))

-            # If input is 1D raise error

-            if array.ndim == 1:

-                raise ValueError(

-                    "Expected 2D array, got 1D array instead:\narray={}.\n"

-                    "Reshape your data either using array.reshape(-1, 1) if "

-                    "your data has a single feature or array.reshape(1, -1) "

-                    "if it contains a single sample.".format(array))

-

-        # in the future np.flexible dtypes will be handled like object dtypes

-        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):

-            warnings.warn(

-                "Beginning in version 0.22, arrays of bytes/strings will be "

-                "converted to decimal numbers if dtype='numeric'. "

-                "It is recommended that you convert the array to "

-                "a float dtype before using it in scikit-learn, "

-                "for example by using "

-                "your_array = your_array.astype(np.float64).",

-                FutureWarning)

-

-        # make sure we actually converted to numeric:

-        if dtype_numeric and array.dtype.kind == "O":

-            array = array.astype(np.float64)

-        if not allow_nd and array.ndim >= 3:

-            raise ValueError("Found array with dim %d. %s expected <= 2."

-                             % (array.ndim, estimator_name))

-        if force_all_finite:

-            _assert_all_finite(array,

-                               allow_nan=force_all_finite == 'allow-nan')

-

-    if ensure_min_samples > 0:

-        n_samples = _num_samples(array)

-        if n_samples < ensure_min_samples:

-            raise ValueError("Found array with %d sample(s) (shape=%s) while a"

-                             " minimum of %d is required%s."

-                             % (n_samples, array.shape, ensure_min_samples,

-                                context))

-

-    if ensure_min_features > 0 and array.ndim == 2:

-        n_features = array.shape[1]

-        if n_features < ensure_min_features:

-            raise ValueError("Found array with %d feature(s) (shape=%s) while"

-                             " a minimum of %d is required%s."

-                             % (n_features, array.shape, ensure_min_features,

-                                context))

-

-    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:

-        msg = ("Data with input dtype %s was converted to %s%s."

-               % (dtype_orig, array.dtype, context))

-        warnings.warn(msg, DataConversionWarning)

-

-    if copy and np.may_share_memory(array, array_orig):

-        array = np.array(array, dtype=dtype, order=order)

-

-    if (warn_on_dtype and dtypes_orig is not None and

-            {array.dtype} != set(dtypes_orig)):

-        # if there was at the beginning some other types than the final one

-        # (for instance in a DataFrame that can contain several dtypes) then

-        # some data must have been converted

-        msg = ("Data with input dtype %s were all converted to %s%s."

-               % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype,

-                  context))

-        warnings.warn(msg, DataConversionWarning, stacklevel=3)

-

-    return array

+        does not match

 

 

 def _check_large_sparse(X, accept_large_sparse=False):


