--- a/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
+++ b/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
@@ -536,106 +536,84 @@
         compiler = q.get_compiler(using=using)

         return '\n'.join(compiler.explain_query())

 

-    def combine(self, rhs, connector):

-        """

-        Merge the 'rhs' query into the current one (with any 'rhs' effects

-        being applied *after* (that is, "to the right of") anything in the

-        current query. 'rhs' is not modified during a call to this function.

-

-        The 'connector' parameter describes how to connect filters from the

-        'rhs' query.

-        """

-        assert self.model == rhs.model, \

-            "Cannot combine queries on two different base models."

-        assert not self.is_sliced, \

-            "Cannot combine queries once a slice has been taken."

-        assert self.distinct == rhs.distinct, \

-            "Cannot combine a unique query with a non-unique query."

-        assert self.distinct_fields == rhs.distinct_fields, \

-            "Cannot combine queries with different distinct fields."

-

-        # Work out how to relabel the rhs aliases, if necessary.

-        change_map = {}

-        conjunction = (connector == AND)

-

-        # Determine which existing joins can be reused. When combining the

-        # query with AND we must recreate all joins for m2m filters. When

-        # combining with OR we can reuse joins. The reason is that in AND

-        # case a single row can't fulfill a condition like:

-        #     revrel__col=1 & revrel__col=2

-        # But, there might be two different related rows matching this

-        # condition. In OR case a single True is enough, so single row is

-        # enough, too.

-        #

-        # Note that we will be creating duplicate joins for non-m2m joins in

-        # the AND case. The results will be correct but this creates too many

-        # joins. This is something that could be fixed later on.

-        reuse = set() if conjunction else set(self.alias_map)

-        # Base table must be present in the query - this is the same

-        # table on both sides.

-        self.get_initial_alias()

-        joinpromoter = JoinPromoter(connector, 2, False)

-        joinpromoter.add_votes(

-            j for j in self.alias_map if self.alias_map[j].join_type == INNER)

-        rhs_votes = set()

-        # Now, add the joins from rhs query into the new query (skipping base

-        # table).

-        rhs_tables = list(rhs.alias_map)[1:]

-        for alias in rhs_tables:

-            join = rhs.alias_map[alias]

-            # If the left side of the join was already relabeled, use the

-            # updated alias.

-            join = join.relabeled_clone(change_map)

-            new_alias = self.join(join, reuse=reuse)

-            if join.join_type == INNER:

-                rhs_votes.add(new_alias)

-            # We can't reuse the same join again in the query. If we have two

-            # distinct joins for the same connection in rhs query, then the

-            # combined query must have two joins, too.

-            reuse.discard(new_alias)

-            if alias != new_alias:

-                change_map[alias] = new_alias

-            if not rhs.alias_refcount[alias]:

-                # The alias was unused in the rhs query. Unref it so that it

-                # will be unused in the new query, too. We have to add and

-                # unref the alias so that join promotion has information of

-                # the join type for the unused alias.

-                self.unref_alias(new_alias)

-        joinpromoter.add_votes(rhs_votes)

-        joinpromoter.update_join_types(self)

-

-        # Now relabel a copy of the rhs where-clause and add it to the current

-        # one.

-        w = rhs.where.clone()

-        w.relabel_aliases(change_map)

-        self.where.add(w, connector)

-

-        # Selection columns and extra extensions are those provided by 'rhs'.

-        if rhs.select:

-            self.set_select([col.relabeled_clone(change_map) for col in rhs.select])

-        else:

-            self.select = ()

-

-        if connector == OR:

-            # It would be nice to be able to handle this, but the queries don't

-            # really make sense (or return consistent value sets). Not worth

-            # the extra complexity when you can write a real query instead.

-            if self.extra and rhs.extra:

-                raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")

-        self.extra.update(rhs.extra)

-        extra_select_mask = set()

-        if self.extra_select_mask is not None:

-            extra_select_mask.update(self.extra_select_mask)

-        if rhs.extra_select_mask is not None:

-            extra_select_mask.update(rhs.extra_select_mask)

-        if extra_select_mask:

-            self.set_extra_mask(extra_select_mask)

-        self.extra_tables += rhs.extra_tables

-

-        # Ordering uses the 'rhs' ordering, unless it has none, in which case

-        # the current ordering is used.

-        self.order_by = rhs.order_by or self.order_by

-        self.extra_order_by = rhs.extra_order_by or self.extra_order_by

+def combine(self, rhs, connector):

+    """

+    Merge the 'rhs' query into the current one (with any 'rhs' effects

+    being applied *after* (that is, "to the right of") anything in the

+    current query. 'rhs' is not modified during a call to this function.

+

+    The 'connector' parameter describes how to connect filters from the

+    'rhs' query.

+    """

+    assert self.model == rhs.model, \

+        "Cannot combine queries on two different base models."

+    assert not self.is_sliced, \

+        "Cannot combine queries once a slice has been taken."

+    assert self.distinct == rhs.distinct, \

+        "Cannot combine a unique query with a non-unique query."

+    assert self.distinct_fields == rhs.distinct_fields, \

+        "Cannot combine queries with different distinct fields."

+

+    # Work out how to relabel the rhs aliases, if necessary.

+    change_map = {}

+    conjunction = (connector == AND)

+

+    # Determine which existing joins can be reused. When combining the

+    # query with AND we must recreate all joins for m2m filters. When

+    # combining with OR we can reuse joins. The reason is that in AND

+    # case a single row can't fulfill a condition like:

+    #     revrel__col=1 & revrel__col=2

+    # But, there might be two different related rows matching this

+    # condition. In OR case a single True is enough, so single row is

+    # enough, too.

+    #

+    # Note that we will be creating duplicate joins for non-m2m joins in

+    # the AND case. The results will be correct but this creates too many

+    # joins. This is something that could be fixed later on.

+    reuse = set() if conjunction else set(self.alias_map)

+    # Base table must be present in the query - this is the same

+    # table on both sides.

+    self.get_initial_alias()

+    joinpromoter = JoinPromoter(connector, 2, False)

+    joinpromoter.add_votes(

+        j for j in self.alias_map if self.alias_map[j].join_type == INNER)

+    rhs_votes = set()

+    # Now, add the joins from rhs query into the new query (skipping base

+    # table).

+    rhs_tables = list(rhs.alias_map)[1:]

+    for alias in rhs_tables:

+        join = rhs.alias_map[alias]

+        # If the left side of the join was already relabeled, use the

+        # updated alias.

+        join = join.relabeled_clone(change_map)

+        new_alias = self.join(join, reuse=reuse)

+        if join.join_type == INNER:

+            rhs_votes.add(new_alias)

+        # We can't reuse the same join again in the query. If we have two

+        # distinct joins for the same connection in rhs query, then the

+        # combined query must have two joins, too.

+        reuse.discard(new_alias)

+        if alias != new_alias:

+            # Append a unique suffix to the alias to ensure uniqueness

+            change_map[alias] = f"{new_alias}_{rhs.model.__name__}"

+        if not rhs.alias_refcount[alias]:

+            # The alias was unused in the rhs query. Unref it so that it

+            # will be unused in the new query, too. We have to add and

+            # unref the alias so that join promotion has information of

+            # the join type for the unused alias.

+            self.unref_alias(new_alias)

+    joinpromoter.add_votes(rhs_votes)

+    joinpromoter.update_join_types(self)

+

+    # Now relabel a copy of the rhs where-clause and add it to the current

+    # one.

+    w = rhs.where.clone()

+    w.relabel_aliases(change_map)

+    self.where.add(w, connector)

+

+    # Selection columns and extra extensions are those provided by 'rhs'.

+    if rhs.select:

+        self.set_select([

 

     def deferred_to_data(self, target, callback):

         """


--- a/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
+++ b/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
@@ -637,91 +637,82 @@
         self.order_by = rhs.order_by or self.order_by

         self.extra_order_by = rhs.extra_order_by or self.extra_order_by

 

-    def deferred_to_data(self, target, callback):

-        """

-        Convert the self.deferred_loading data structure to an alternate data

-        structure, describing the field that *will* be loaded. This is used to

-        compute the columns to select from the database and also by the

-        QuerySet class to work out which fields are being initialized on each

-        model. Models that have all their fields included aren't mentioned in

-        the result, only those that have field restrictions in place.

-

-        The "target" parameter is the instance that is populated (in place).

-        The "callback" is a function that is called whenever a (model, field)

-        pair need to be added to "target". It accepts three parameters:

-        "target", and the model and list of fields being added for that model.

-        """

-        field_names, defer = self.deferred_loading

-        if not field_names:

-            return

-        orig_opts = self.get_meta()

-        seen = {}

-        must_include = {orig_opts.concrete_model: {orig_opts.pk}}

-        for field_name in field_names:

-            parts = field_name.split(LOOKUP_SEP)

-            cur_model = self.model._meta.concrete_model

-            opts = orig_opts

-            for name in parts[:-1]:

-                old_model = cur_model

-                if name in self._filtered_relations:

-                    name = self._filtered_relations[name].relation_name

-                source = opts.get_field(name)

-                if is_reverse_o2o(source):

-                    cur_model = source.related_model

-                else:

-                    cur_model = source.remote_field.model

-                opts = cur_model._meta

-                # Even if we're "just passing through" this model, we must add

-                # both the current model's pk and the related reference field

-                # (if it's not a reverse relation) to the things we select.

-                if not is_reverse_o2o(source):

-                    must_include[old_model].add(source)

-                add_to_dict(must_include, cur_model, opts.pk)

-            field = opts.get_field(parts[-1])

-            is_reverse_object = field.auto_created and not field.concrete

-            model = field.related_model if is_reverse_object else field.model

-            model = model._meta.concrete_model

-            if model == opts.model:

-                model = cur_model

-            if not is_reverse_o2o(field):

-                add_to_dict(seen, model, field)

-

-        if defer:

-            # We need to load all fields for each model, except those that

-            # appear in "seen" (for all models that appear in "seen"). The only

-            # slight complexity here is handling fields that exist on parent

-            # models.

-            workset = {}

-            for model, values in seen.items():

-                for field in model._meta.local_fields:

-                    if field not in values:

-                        m = field.model._meta.concrete_model

-                        add_to_dict(workset, m, field)

-            for model, values in must_include.items():

-                # If we haven't included a model in workset, we don't add the

-                # corresponding must_include fields for that model, since an

-                # empty set means "include all fields". That's why there's no

-                # "else" branch here.

-                if model in workset:

-                    workset[model].update(values)

-            for model, values in workset.items():

-                callback(target, model, values)

-        else:

-            for model, values in must_include.items():

-                if model in seen:

-                    seen[model].update(values)

-                else:

-                    # As we've passed through this model, but not explicitly

-                    # included any fields, we have to make sure it's mentioned

-                    # so that only the "must include" fields are pulled in.

-                    seen[model] = values

-            # Now ensure that every model in the inheritance chain is mentioned

-            # in the parent list. Again, it must be mentioned to ensure that

-            # only "must include" fields are pulled in.

-            for model in orig_opts.get_parent_list():

-                seen.setdefault(model, set())

-            for model, values in seen.items():

-                callback(target, model, values)

+def deferred_to_data(self, target, callback):

+    """

+    Convert the self.deferred_loading data structure to an alternate data

+    structure, describing the field that *will* be loaded. This is used to

+    compute the columns to select from the database and also by the

+    QuerySet class to work out which fields are being initialized on each

+    model. Models that have all their fields included aren't mentioned in

+    the result, only those that have field restrictions in place.

+

+    The "target" parameter is the instance that is populated (in place).

+    The "callback" is a function that is called whenever a (model, field)

+    pair need to be added to "target". It accepts three parameters:

+    "target", and the model and list of fields being added for that model.

+    """

+    field_names, defer = self.deferred_loading

+    if not field_names:

+        return

+    orig_opts = self.get_meta()

+    seen = {}

+    must_include = {orig_opts.concrete_model: {orig_opts.pk}}

+    for field_name in field_names:

+        parts = field_name.split(LOOKUP_SEP)

+        cur_model = self.model._meta.concrete_model

+        opts = orig_opts

+        for name in parts[:-1]:

+            old_model = cur_model

+            if name in self._filtered_relations:

+                name = self._filtered_relations[name].relation_name

+            source = opts.get_field(name)

+            if is_reverse_o2o(source):

+                cur_model = source.related_model

+            else:

+                cur_model = source.remote_field.model

+            opts = cur_model._meta

+            # Even if we're "just passing through" this model, we must add

+            # both the current model's pk and the related reference field

+            # (if it's not a reverse relation) to the things we select.

+            if not is_reverse_o2o(source):

+                must_include[old_model].add(source)

+            add_to_dict(must_include, cur_model, opts.pk)

+        field = opts.get_field(parts[-1])

+        is_reverse_object = field.auto_created and not field.concrete

+        model = field.related_model if is_reverse_object else field.model

+        model = model._meta.concrete_model

+        if model == opts.model:

+            model = cur_model

+        if not is_reverse_o2o(field):

+            add_to_dict(seen, model, field)

+

+    if defer:

+        # We need to load all fields for each model, except those that

+        # appear in "seen" (for all models that appear in "seen"). The only

+        # slight complexity here is handling fields that exist on parent

+        # models.

+        workset = {}

+        for model, values in seen.items():

+            for field in model._meta.local_fields:

+                if field not in values:

+                    m = field.model._meta.concrete_model

+                    add_to_dict(workset, m, field)

+        for model, values in must_include.items():

+            # If we haven't included a model in workset, we don't add the

+            # corresponding must_include fields for that model, since an

+            # empty set means "include all fields". That's why there's no

+            # "else" branch here.

+            if model in workset:

+                workset[model].update(values)

+        for model, values in workset.items():

+            callback(target, model, values)

+    else:

+        for model, values in must_include.items():

+            if model in seen:

+                seen[model].update(values)

+            else:

+                # As we've passed through this model, but not explicitly

+                # included any fields, we have to make sure

 

     def table_alias(self, table_name, create=False, filtered_relation=None):

         """


--- a/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
+++ b/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
@@ -2202,7 +2202,7 @@
         self.set_select([f.get_col(select_alias) for f in select_fields])

         return trimmed_prefix, contains_louter

 

-    def is_nullable(self, field):

+def is_nullable(self, field):

         """

         Check if the given field should be treated as nullable.

 

@@ -2210,11 +2210,9 @@
         nullable for those backends. In such situations field.null can be

         False even if we should treat the field as nullable.

         """

-        # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have

-        # (nor should it have) knowledge of which connection is going to be

-        # used. The proper fix would be to defer all decisions where

-        # is_nullable() is needed to the compiler stage, but that is not easy

-        # to do currently.

+        # This function does not affect table names and is used to determine

+        # if a field should be treated as nullable. It does not cause table

+        # name conflicts.

         return (

             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and

             field.empty_strings_allowed


--- a/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
+++ b/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
@@ -756,39 +756,39 @@
         """Decreases the reference count for this alias."""

         self.alias_refcount[alias] -= amount

 

-    def promote_joins(self, aliases):

-        """

-        Promote recursively the join type of given aliases and its children to

-        an outer join. If 'unconditional' is False, only promote the join if

-        it is nullable or the parent join is an outer join.

-

-        The children promotion is done to avoid join chains that contain a LOUTER

-        b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,

-        then we must also promote b->c automatically, or otherwise the promotion

-        of a->b doesn't actually change anything in the query results.

-        """

-        aliases = list(aliases)

-        while aliases:

-            alias = aliases.pop(0)

-            if self.alias_map[alias].join_type is None:

-                # This is the base table (first FROM entry) - this table

-                # isn't really joined at all in the query, so we should not

-                # alter its join type.

-                continue

-            # Only the first alias (skipped above) should have None join_type

-            assert self.alias_map[alias].join_type is not None

-            parent_alias = self.alias_map[alias].parent_alias

-            parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER

-            already_louter = self.alias_map[alias].join_type == LOUTER

-            if ((self.alias_map[alias].nullable or parent_louter) and

-                    not already_louter):

-                self.alias_map[alias] = self.alias_map[alias].promote()

-                # Join type of 'alias' changed, so re-examine all aliases that

-                # refer to this one.

-                aliases.extend(

-                    join for join in self.alias_map

-                    if self.alias_map[join].parent_alias == alias and join not in aliases

-                )

+def promote_joins(self, aliases):

+    """

+    Promote recursively the join type of given aliases and its children to

+    an outer join. If 'unconditional' is False, only promote the join if

+    it is nullable or the parent join is an outer join.

+

+    The children promotion is done to avoid join chains that contain a LOUTER

+    b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,

+    then we must also promote b->c automatically, or otherwise the promotion

+    of a->b doesn't actually change anything in the query results.

+    """

+    aliases = list(aliases)

+    while aliases:

+        alias = aliases.pop(0)

+        if self.alias_map[alias].join_type is None:

+            # This is the base table (first FROM entry) - this table

+            # isn't really joined at all in the query, so we should not

+            # alter its join type.

+            continue

+        # Only the first alias (skipped above) should have None join_type

+        assert self.alias_map[alias].join_type is not None

+        parent_alias = self.alias_map[alias].parent_alias

+        parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER

+        already_louter = self.alias_map[alias].join_type == LOUTER

+        if ((self.alias_map[alias].nullable or parent_louter) and

+                not already_louter):

+            self.alias_map[alias] = self.alias_map[alias].promote()

+            # Join type of 'alias' changed, so re-examine all aliases that

+            # refer to this one.

+            aliases.extend(

+                join for join in self.alias_map

+                if self.alias_map[join].parent_alias == alias and join not in aliases

+            )

 

     def demote_joins(self, aliases):

         """


--- a/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
+++ b/decoupled/60/0.8_0.2/20/django_django/django/db/models/sql/query.py
@@ -2013,24 +2013,24 @@
             # Replace any existing "immediate load" field names.

             self.deferred_loading = frozenset(field_names), False

 

-    def get_loaded_field_names(self):

-        """

-        If any fields are marked to be deferred, return a dictionary mapping

-        models to a set of names in those fields that will be loaded. If a

-        model is not in the returned dictionary, none of its fields are

-        deferred.

-

-        If no fields are marked for deferral, return an empty dictionary.

-        """

-        # We cache this because we call this function multiple times

-        # (compiler.fill_related_selections, query.iterator)

-        try:

-            return self._loaded_field_names_cache

-        except AttributeError:

-            collection = {}

-            self.deferred_to_data(collection, self.get_loaded_field_names_cb)

-            self._loaded_field_names_cache = collection

-            return collection

+def get_loaded_field_names(self):

+    """

+    If any fields are marked to be deferred, return a dictionary mapping

+    models to a set of names in those fields that will be loaded. If a

+    model is not in the returned dictionary, none of its fields are

+    deferred.

+

+    If no fields are marked for deferral, return an empty dictionary.

+    """

+    # We cache this because we call this function multiple times

+    # (compiler.fill_related_selections, query.iterator)

+    try:

+        return self._loaded_field_names_cache

+    except AttributeError:

+        collection = {}

+        self.deferred_to_data(collection, self.get_loaded_field_names_cb)

+        self._loaded_field_names_cache = collection

+        return collection

 

     def get_loaded_field_names_cb(self, target, model, fields):

         """Callback used by get_deferred_field_names()."""


