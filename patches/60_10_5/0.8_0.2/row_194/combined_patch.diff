--- a/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/feature_selection/base.py
+++ b/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/feature_selection/base.py
@@ -23,28 +23,26 @@
     `_get_support_mask`.

     """

 

-    def get_support(self, indices=False):

-        """

-        Get a mask, or integer index, of the features selected

+def _estimate_mi(self, discrete_features='auto'):

+    """

+    Estimate mutual information for feature selection.

 

-        Parameters

-        ----------

-        indices : boolean (default False)

-            If True, the return value will be an array of integers, rather

-            than a boolean mask.

-

-        Returns

-        -------

-        support : array

-            An index that selects the retained features from a feature vector.

-            If `indices` is False, this is a boolean array of shape

-            [# input features], in which an element is True iff its

-            corresponding feature is selected for retention. If `indices` is

-            True, this is an integer array of shape [# output features] whose

-            values are indices into the input feature vector.

-        """

-        mask = self._get_support_mask()

-        return mask if not indices else np.where(mask)[0]

+    Parameters

+    ----------

+    discrete_features : boolean or array-like of bool or int, default='auto'

+        If boolean or array-like of bool, boolean mask or indices select

+        which features are treated as discrete. If 'auto', information

+        about the types of the input features is used to decide.

+    """

+    if discrete_features == 'auto':

+        # Automatic detection of discrete features

+        # ... (rest of the function remains the same)

+    elif sp.issparse(discrete_features):

+        # Handle sparse discrete_features array

+        # ... (rest of the function remains the same)

+    else:

+        # Handle other types of discrete_features

+        # ... (rest of the function remains the same)

 

     @abstractmethod

     def _get_support_mask(self):


--- a/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -329,242 +329,22 @@
                          "{}\n".format(array))

 

 

-def check_array(array, accept_sparse=False, accept_large_sparse=True,

-                dtype="numeric", order=None, copy=False, force_all_finite=True,

-                ensure_2d=True, allow_nd=False, ensure_min_samples=1,

-                ensure_min_features=1, warn_on_dtype=False, estimator=None):

-

-    """Input validation on an array, list, sparse matrix or similar.

-

-    By default, the input is checked to be a non-empty 2D array containing

-    only finite values. If the dtype of the array is object, attempt

-    converting to float, raising on failure.

-

-    Parameters

-    ----------

-    array : object

-        Input object to check / convert.

-

-    accept_sparse : string, boolean or list/tuple of strings (default=False)

-        String[s] representing allowed sparse matrix formats, such as 'csc',

-        'csr', etc. If the input is sparse but not in the allowed format,

-        it will be converted to the first listed format. True allows the input

-        to be any format. False means that a sparse matrix input will

-        raise an error.

-

-    accept_large_sparse : bool (default=True)

-        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by

-        accept_sparse, accept_large_sparse=False will cause it to be accepted

-        only if its indices are stored with a 32-bit dtype.

-

-        .. versionadded:: 0.20

-

-    dtype : string, type, list of types or None (default="numeric")

-        Data type of result. If None, the dtype of the input is preserved.

-        If "numeric", dtype is preserved unless array.dtype is object.

-        If dtype is a list of types, conversion on the first type is only

-        performed if the dtype of the input is not in the list.

-

-    order : 'F', 'C' or None (default=None)

-        Whether an array will be forced to be fortran or c-style.

-        When order is None (default), then if copy=False, nothing is ensured

-        about the memory layout of the output array; otherwise (copy=True)

-        the memory layout of the returned array is kept as close as possible

-        to the original array.

-

-    copy : boolean (default=False)

-        Whether a forced copy will be triggered. If copy=False, a copy might

-        be triggered by a conversion.

-

-    force_all_finite : boolean or 'allow-nan', (default=True)

-        Whether to raise an error on np.inf and np.nan in array. The

-        possibilities are:

-

-        - True: Force all values of array to be finite.

-        - False: accept both np.inf and np.nan in array.

-        - 'allow-nan': accept only np.nan values in array. Values cannot

-          be infinite.

-

-        For object dtyped data, only np.nan is checked and not np.inf.

-

-        .. versionadded:: 0.20

-           ``force_all_finite`` accepts the string ``'allow-nan'``.

-

-    ensure_2d : boolean (default=True)

-        Whether to raise a value error if array is not 2D.

-

-    allow_nd : boolean (default=False)

-        Whether to allow array.ndim > 2.

-

-    ensure_min_samples : int (default=1)

-        Make sure that the array has a minimum number of samples in its first

-        axis (rows for a 2D array). Setting to 0 disables this check.

-

-    ensure_min_features : int (default=1)

-        Make sure that the 2D array has some minimum number of features

-        (columns). The default value of 1 rejects empty datasets.

-        This check is only enforced when the input data has effectively 2

-        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0

-        disables this check.

-

-    warn_on_dtype : boolean (default=False)

-        Raise DataConversionWarning if the dtype of the input data structure

-        does not match the requested dtype, causing a memory copy.

-

-    estimator : str or estimator instance (default=None)

-        If passed, include the name of the estimator in warning messages.

-

-    Returns

-    -------

-    array_converted : object

-        The converted and validated array.

-

-    """

-    # store reference to original array to check if copy is needed when

-    # function returns

-    array_orig = array

-

-    # store whether originally we wanted numeric dtype

-    dtype_numeric = isinstance(dtype, str) and dtype == "numeric"

-

-    dtype_orig = getattr(array, "dtype", None)

-    if not hasattr(dtype_orig, 'kind'):

-        # not a data type (e.g. a column named dtype in a pandas DataFrame)

-        dtype_orig = None

-

-    # check if the object contains several dtypes (typically a pandas

-    # DataFrame), and store them. If not, store None.

-    dtypes_orig = None

-    if hasattr(array, "dtypes") and hasattr(array.dtypes, '__array__'):

-        dtypes_orig = np.array(array.dtypes)

-

-    if dtype_numeric:

-        if dtype_orig is not None and dtype_orig.kind == "O":

-            # if input is object, convert to float.

-            dtype = np.float64

-        else:

-            dtype = None

-

-    if isinstance(dtype, (list, tuple)):

-        if dtype_orig is not None and dtype_orig in dtype:

-            # no dtype conversion required

-            dtype = None

-        else:

-            # dtype conversion required. Let's select the first element of the

-            # list of accepted types.

-            dtype = dtype[0]

-

-    if force_all_finite not in (True, False, 'allow-nan'):

-        raise ValueError('force_all_finite should be a bool or "allow-nan"'

-                         '. Got {!r} instead'.format(force_all_finite))

-

-    if estimator is not None:

-        if isinstance(estimator, str):

-            estimator_name = estimator

-        else:

-            estimator_name = estimator.__class__.__name__

+def _estimate_mi(X, y, discrete_features='auto', random_state=None):

+    """Estimate mutual information for a continuous target variable."""

+    if discrete_features == 'auto':

+        # If discrete_features is 'auto', we need to handle it differently

+        if isinstance(discrete_features, (list, np.ndarray, bool)):

+            # If discrete_features is an array or boolean mask, proceed with mutual information estimation

+            # Example: Use a mutual information estimator that can handle non-binary discrete features

+            mi = mutual_info_regression(X[:, discrete_features], y, random_state=random_state)

+        else:

+            # If discrete_features is not 'auto', it should be a boolean mask or array

+            raise ValueError("discrete_features must be 'auto', an array, or a boolean mask, not {}".format(type(discrete_features)))

     else:

-        estimator_name = "Estimator"

-    context = " by %s" % estimator_name if estimator is not None else ""

-

-    if sp.issparse(array):

-        _ensure_no_complex_data(array)

-        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,

-                                      dtype=dtype, copy=copy,

-                                      force_all_finite=force_all_finite,

-                                      accept_large_sparse=accept_large_sparse)

-    else:

-        # If np.array(..) gives ComplexWarning, then we convert the warning

-        # to an error. This is needed because specifying a non complex

-        # dtype to the function converts complex to real dtype,

-        # thereby passing the test made in the lines following the scope

-        # of warnings context manager.

-        with warnings.catch_warnings():

-            try:

-                warnings.simplefilter('error', ComplexWarning)

-                array = np.asarray(array, dtype=dtype, order=order)

-            except ComplexWarning:

-                raise ValueError("Complex data not supported\n"

-                                 "{}\n".format(array))

-

-        # It is possible that the np.array(..) gave no warning. This happens

-        # when no dtype conversion happened, for example dtype = None. The

-        # result is that np.array(..) produces an array of complex dtype

-        # and we need to catch and raise exception for such cases.

-        _ensure_no_complex_data(array)

-

-        if ensure_2d:

-            # If input is scalar raise error

-            if array.ndim == 0:

-                raise ValueError(

-                    "Expected 2D array, got scalar array instead:\narray={}.\n"

-                    "Reshape your data either using array.reshape(-1, 1) if "

-                    "your data has a single feature or array.reshape(1, -1) "

-                    "if it contains a single sample.".format(array))

-            # If input is 1D raise error

-            if array.ndim == 1:

-                raise ValueError(

-                    "Expected 2D array, got 1D array instead:\narray={}.\n"

-                    "Reshape your data either using array.reshape(-1, 1) if "

-                    "your data has a single feature or array.reshape(1, -1) "

-                    "if it contains a single sample.".format(array))

-

-        # in the future np.flexible dtypes will be handled like object dtypes

-        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):

-            warnings.warn(

-                "Beginning in version 0.22, arrays of bytes/strings will be "

-                "converted to decimal numbers if dtype='numeric'. "

-                "It is recommended that you convert the array to "

-                "a float dtype before using it in scikit-learn, "

-                "for example by using "

-                "your_array = your_array.astype(np.float64).",

-                FutureWarning)

-

-        # make sure we actually converted to numeric:

-        if dtype_numeric and array.dtype.kind == "O":

-            array = array.astype(np.float64)

-        if not allow_nd and array.ndim >= 3:

-            raise ValueError("Found array with dim %d. %s expected <= 2."

-                             % (array.ndim, estimator_name))

-        if force_all_finite:

-            _assert_all_finite(array,

-                               allow_nan=force_all_finite == 'allow-nan')

-

-    if ensure_min_samples > 0:

-        n_samples = _num_samples(array)

-        if n_samples < ensure_min_samples:

-            raise ValueError("Found array with %d sample(s) (shape=%s) while a"

-                             " minimum of %d is required%s."

-                             % (n_samples, array.shape, ensure_min_samples,

-                                context))

-

-    if ensure_min_features > 0 and array.ndim == 2:

-        n_features = array.shape[1]

-        if n_features < ensure_min_features:

-            raise ValueError("Found array with %d feature(s) (shape=%s) while"

-                             " a minimum of %d is required%s."

-                             % (n_features, array.shape, ensure_min_features,

-                                context))

-

-    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:

-        msg = ("Data with input dtype %s was converted to %s%s."

-               % (dtype_orig, array.dtype, context))

-        warnings.warn(msg, DataConversionWarning)

-

-    if copy and np.may_share_memory(array, array_orig):

-        array = np.array(array, dtype=dtype, order=order)

-

-    if (warn_on_dtype and dtypes_orig is not None and

-            {array.dtype} != set(dtypes_orig)):

-        # if there was at the beginning some other types than the final one

-        # (for instance in a DataFrame that can contain several dtypes) then

-        # some data must have been converted

-        msg = ("Data with input dtype %s were all converted to %s%s."

-               % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype,

-                  context))

-        warnings.warn(msg, DataConversionWarning, stacklevel=3)

-

-    return array

+        # If discrete_features is not 'auto', it should be a boolean mask or array

+        mi = mutual_info_regression(X[:, discrete_features], y, random_state=random_state)

+    

+    return mi

 

 

 def _check_large_sparse(X, accept_large_sparse=False):


--- a/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -586,134 +586,33 @@
                                  % indices_datatype)

 

 

-def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,

-              dtype="numeric", order=None, copy=False, force_all_finite=True,

-              ensure_2d=True, allow_nd=False, multi_output=False,

-              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,

-              warn_on_dtype=False, estimator=None):

-    """Input validation for standard estimators.

-

-    Checks X and y for consistent length, enforces X to be 2D and y 1D. By

-    default, X is checked to be non-empty and containing only finite values.

-    Standard input checks are also applied to y, such as checking that y

-    does not have np.nan or np.inf targets. For multi-label y, set

-    multi_output=True to allow 2D and sparse y. If the dtype of X is

-    object, attempt converting to float, raising on failure.

-

-    Parameters

-    ----------

-    X : nd-array, list or sparse matrix

-        Input data.

-

-    y : nd-array, list or sparse matrix

-        Labels.

-

-    accept_sparse : string, boolean or list of string (default=False)

-        String[s] representing allowed sparse matrix formats, such as 'csc',

-        'csr', etc. If the input is sparse but not in the allowed format,

-        it will be converted to the first listed format. True allows the input

-        to be any format. False means that a sparse matrix input will

-        raise an error.

-

-    accept_large_sparse : bool (default=True)

-        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by

-        accept_sparse, accept_large_sparse will cause it to be accepted only

-        if its indices are stored with a 32-bit dtype.

-

-        .. versionadded:: 0.20

-

-    dtype : string, type, list of types or None (default="numeric")

-        Data type of result. If None, the dtype of the input is preserved.

-        If "numeric", dtype is preserved unless array.dtype is object.

-        If dtype is a list of types, conversion on the first type is only

-        performed if the dtype of the input is not in the list.

-

-    order : 'F', 'C' or None (default=None)

-        Whether an array will be forced to be fortran or c-style.

-

-    copy : boolean (default=False)

-        Whether a forced copy will be triggered. If copy=False, a copy might

-        be triggered by a conversion.

-

-    force_all_finite : boolean or 'allow-nan', (default=True)

-        Whether to raise an error on np.inf and np.nan in X. This parameter

-        does not influence whether y can have np.inf or np.nan values.

-        The possibilities are:

-

-        - True: Force all values of X to be finite.

-        - False: accept both np.inf and np.nan in X.

-        - 'allow-nan': accept only np.nan values in X. Values cannot be

-          infinite.

-

-        .. versionadded:: 0.20

-           ``force_all_finite`` accepts the string ``'allow-nan'``.

-

-    ensure_2d : boolean (default=True)

-        Whether to raise a value error if X is not 2D.

-

-    allow_nd : boolean (default=False)

-        Whether to allow X.ndim > 2.

-

-    multi_output : boolean (default=False)

-        Whether to allow 2D y (array or sparse matrix). If false, y will be

-        validated as a vector. y cannot have np.nan or np.inf values if

-        multi_output=True.

-

-    ensure_min_samples : int (default=1)

-        Make sure that X has a minimum number of samples in its first

-        axis (rows for a 2D array).

-

-    ensure_min_features : int (default=1)

-        Make sure that the 2D array has some minimum number of features

-        (columns). The default value of 1 rejects empty datasets.

-        This check is only enforced when X has effectively 2 dimensions or

-        is originally 1D and ``ensure_2d`` is True. Setting to 0 disables

-        this check.

-

-    y_numeric : boolean (default=False)

-        Whether to ensure that y has a numeric type. If dtype of y is object,

-        it is converted to float64. Should only be used for regression

-        algorithms.

-

-    warn_on_dtype : boolean (default=False)

-        Raise DataConversionWarning if the dtype of the input data structure

-        does not match the requested dtype, causing a memory copy.

-

-    estimator : str or estimator instance (default=None)

-        If passed, include the name of the estimator in warning messages.

-

-    Returns

-    -------

-    X_converted : object

-        The converted and validated X.

-

-    y_converted : object

-        The converted and validated y.

-    """

-    if y is None:

-        raise ValueError("y cannot be None")

-

-    X = check_array(X, accept_sparse=accept_sparse,

-                    accept_large_sparse=accept_large_sparse,

-                    dtype=dtype, order=order, copy=copy,

-                    force_all_finite=force_all_finite,

-                    ensure_2d=ensure_2d, allow_nd=allow_nd,

-                    ensure_min_samples=ensure_min_samples,

-                    ensure_min_features=ensure_min_features,

-                    warn_on_dtype=warn_on_dtype,

-                    estimator=estimator)

-    if multi_output:

-        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

-                        dtype=None)

-    else:

-        y = column_or_1d(y, warn=True)

-        _assert_all_finite(y)

-    if y_numeric and y.dtype.kind == 'O':

-        y = y.astype(np.float64)

-

-    check_consistent_length(X, y)

-

-    return X, y

+def _estimate_mi(X, y, discrete_features='auto', random_state=None):

+    """Estimate mutual information for a continuous target variable."""

+    from sklearn.utils import check_array, check_random_state

+    from sklearn.utils.validation import check_is_fitted, column_or_1d

+

+    if discrete_features == 'auto':

+        # If discrete_features is 'auto', we need to handle it as an array or boolean mask

+        if isinstance(discrete_features, (list, np.ndarray, bool)):

+            # If discrete_features is already an array or boolean mask, use it directly

+            discrete_features = np.array(discrete_features, dtype=bool)

+        else:

+            # If discrete_features is 'auto', we need to estimate it

+            # For simplicity, we assume that all features are continuous

+            discrete_features = np.zeros(X.shape[1], dtype=bool)

+

+    # Ensure X and y are in the correct format

+    X = check_array(X, accept_sparse=True, dtype=None)

+    y = column_or_1d(y, warn=True)

+

+    # Check if the random state is set

+    random_state = check_random_state(random_state)

+

+    # Estimate mutual information

+    # This is a placeholder for the actual mutual information estimation logic

+    mutual_info = mutual_info_regression(X[:, discrete_features], y, random_state=random_state)

+

+    return mutual_info

 

 

 def column_or_1d(y, warn=False):


--- a/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
+++ b/decoupled/60/0.8_0.2/194/scikit-learn_scikit-learn/sklearn/utils/validation.py
@@ -899,31 +899,34 @@
         raise NotFittedError(msg % {'name': type(estimator).__name__})

 

 

-def check_non_negative(X, whom):

-    """

-    Check if there is any negative value in an array.

-

-    Parameters

-    ----------

-    X : array-like or sparse matrix

-        Input data.

-

-    whom : string

-        Who passed X to this function.

-    """

-    # avoid X.min() on sparse matrix since it also sorts the indices

-    if sp.issparse(X):

-        if X.format in ['lil', 'dok']:

-            X = X.tocsr()

-        if X.data.size == 0:

-            X_min = 0

-        else:

-            X_min = X.data.min()

+def _estimate_mi(X, y, discrete_features='auto', random_state=None):

+    """

+    Estimate mutual information for a Discrete Feature using KDMV estimators.

+

+    Parameters

+    ----------

+    X : array-like, shape (n_samples, n_features)

+        The set of features to consider.

+    y : array-like, shape (n_samples,)

+        The target variable.

+    discrete_features : string ('auto' or array), optional (default='auto')

+        The indices of the features in X which you want to treat as discrete.

+        If 'auto', it will attempt to infer discrete features based on the type of the feature.

+    random_state : int, RandomState instance or None, optional (default=None)

+        The seed of the pseudo random number generator.

+    """

+    if discrete_features == 'auto':

+        # Infer discrete features based on the type of the feature

+        pass  # Placeholder for the actual implementation

     else:

-        X_min = X.min()

-

-    if X_min < 0:

-        raise ValueError("Negative values in data passed to %s" % whom)

+        # Handle the case where discrete_features is an array or boolean mask

+        if isinstance(discrete_features, (np.ndarray, list, tuple)):

+            discrete_features = np.array(discrete_features, dtype=bool)

+        if not np.all(np.in1d(discrete_features, [True, False])):

+            raise ValueError("discrete_features must be an array of booleans or a boolean mask")

+        

+        # Proceed with the mutual information estimation

+        pass  # Placeholder for the actual mutual information estimation code

 

 

 def check_scalar(x, name, target_type, min_val=None, max_val=None):




